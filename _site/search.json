[
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Enriching independent data with industry-relevant concepts\n\n\nUse case of adding container shipping aspects to AIS data\n\n\n\n\ndata science\n\n\nmaritime\n\n\nshipping\n\n\nais\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nRAPping in the public sector\n\n\n\n\n\n\n\ndata science\n\n\nreproducibility\n\n\npublic sector\n\n\npipelines\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2021\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nInfrastructure Flows\n\n\n\n\n\n\n\ndata science\n\n\nrail\n\n\ntransport\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2021\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nIdentifying spatial network issues\n\n\n\n\n\n\n\ndata science\n\n\nrail\n\n\ntransport\n\n\nnetwork analysis\n\n\nnew zealand\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nModelling - a brief reflection\n\n\n\n\n\n\n\ndata science\n\n\nmodelling\n\n\nstatistics\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2020\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nWalking in Wellington - insights from modelling\n\n\nPart 4: Bayesian modelling of walkability\n\n\n\n\ndata science\n\n\nwalkability\n\n\ntransport\n\n\nnew zealand\n\n\nnetwork analysis\n\n\nmodelling\n\n\nstatistics\n\n\nbayesian\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2020\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nWalking in Wellington - validation and visualisation\n\n\nPart 3: Checking network results with APIs and visualising walkability isochrones\n\n\n\n\ndata science\n\n\nwalkability\n\n\ntransport\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2020\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nWalking in Wellington - walkability metrics\n\n\nPart 2: Calculating elevation-sensitve walkability\n\n\n\n\ndata science\n\n\nwalkability\n\n\ntransport\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nWalking in Wellington\n\n\nPart 1: why analyse walkability?\n\n\n\n\ndata science\n\n\nwalkability\n\n\ntransport\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial proximity analysis with fuel stations - data\n\n\nPart 1: using OpenStreetMap to get data\n\n\n\n\ndata science\n\n\nurban\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2018\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial proximity analysis with fuel stations - accessibility\n\n\nPart 3: customer accessibility\n\n\n\n\ndata science\n\n\nurban\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2018\n\n\nShrividya Ravi\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial proximity analysis with fuel stations - networks\n\n\nPart 2: fuel station proximity networks\n\n\n\n\ndata science\n\n\nurban\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2018\n\n\nShrividya Ravi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hierarchical-size-models/index.html",
    "href": "projects/hierarchical-size-models/index.html",
    "title": "Modelling the real world with hierarchical models",
    "section": "",
    "text": "A simple model usually assumes no grouping or structure in the data. However, real world processes often have clear hierarchy and structure. For example, Brand A shirts will be more similar to Brand B shirts than Brand A trousers. Thus, a natural extension to modelling fit is to incorporate a hierarchical structure allows that modelling of categories that closely emulate reality like garment type, fit type (slim, regular, plus) or brand. Using this underlying structure I was able to build a collective size model for a garment category and individual models for each brand.\n\nThe advantage of hierarchical modelling is that the population average becomes a prior for all the groups. This results in an effect called shrinkage - where groups with low observations are more strongly pulled towards the population values. The shrinkage effect is very useful in real world applications since the advice given to customers maintains an average statsus quo until there is sufficient data to deviate.\nMore than one level in the model hierarchy requires a re-structuring of the model as a ‘multilevel’ structure - where all the levels are a shift from a mean value. I built an extension of the model described earlier - unifying a collection of models for the different garment types (with underlying structure by brand) into a single model with coefficients for both garment type and brand.\nA forest plot visualisation of fit coefficients at the different levels can offer both quantitative and domain insight. In fit terms the plot can be interpreted as knitwear having more tolerance and generally accomodating more shapes while shorts and coats fit smaller than the average across all garments."
  },
  {
    "objectID": "projects/returns-prediction/index.html",
    "href": "projects/returns-prediction/index.html",
    "title": "Predicting customer behaviour: returns",
    "section": "",
    "text": "Returns are an inevitable consequence of spatial and temporal separation of purchase from physical interation with the item. Despite advanced multimedia in presenting item characteristics to the customer, there is still a fundamental lack of tactile understanding. The importance of such an understanding is both a function of customer preference and item characteristics. The physical feel and wear of some items are inherently harder to communicate and, customers’ themselves have different preferences of what they expect from the photos and videos. As a result, returns are usually distributed. The shape and extremes of returns are, of course, variable depending on the item type and the brand / retailer!\nThe level of variability in returns prompts a desire to understand why returns happen in the first place. For garments at a particular retailer, I built two simple returns models to disseminate how items and customer characteristics contribute to when returns happen. The model (left figure) with item characteristics alone showed that returns are more unpredictable than expected. Including features based on customer behaviours (right figure) lift prediction accuracy considerably.\nWorking on such an intersting domain problem highlighted the importance of complext interactions and intenstions. For example, any outcome of behaviour requires not only a sense of the customer’s intent when shopping but also their interaction and attitudes towards the browsed items. Had the project continued for longer, the creation of customer preferences, intentions and interactions would have been the next step in building a better model."
  },
  {
    "objectID": "projects/shipping-schedules/index.html",
    "href": "projects/shipping-schedules/index.html",
    "title": "Shipping Schedules",
    "section": "",
    "text": "One of the challenges in the public sector is getting high level understanding of operational aspects without requiring commercially sensitive data or extensive negotiations to get consistent data from many commericial providers.\nSince the COVID-19 pandemic, policymakers have needed information on ship schedules to and from New Zealand to derive contextual understanding of trade capacity over time without the specificity of commercial offerings from shipping lines.\nEnriching a comprehensive data source like AIS (Automatic Identification System) with industry-relevant concepts like voyages and routes (see blog post) allows us to build retrospective ship schedules from a single data source. Simple visualisations summarise the relevant information for policymakers to easily compare ships doing differents and the proportion of time they spend on a voyage that includes New Zealand.\nFor example, we see that around a third of ships that made any Trans-Tasman (Australia-New Zealand) voyage in 2021 also do the much longer Latin America-New Zealand-Northern America routes in between. Only a handful of ships do purely Trans-Tasman voyages for an extended period of time.\n\nA completely different pattern exists for ships that have done a Australia-New Zealand-Southeastern Asia route. For these ships, around half run regularly on this route route while another quarter run a variant that only drops Australian ports on the voyage.\n\n\nDisclaimer\nThe contents and figures in this post are not official outputs from the Ministry of Transport. They are research-oriented exploratory analyses intended as demonstrations of approaches and techniques relevant to public sector data science."
  },
  {
    "objectID": "projects/long-conversion/index.html",
    "href": "projects/long-conversion/index.html",
    "title": "Long tail behaviours",
    "section": "",
    "text": "Customer analytics is a multidisciplinary field of marketing techniques, causal methods and market research - mostly towards the aim of getting us to open our wallets a little more. While this aim is ethically ambiguous to some, the technical side is rich and ripe for bringing in statisitcal techniques used extensively in other disciplines for additional information and insight.\nOne useful method for working with customer data is survival analysis. Heavily used within insurance, it is a useful set of models that take time series data of activity and return a probability of ‘survival’ to that point in time, and continued survival. For example, we can take a time series of a customer’s visits to a website and get their probability over time of being a visitor.\nSurvival analysis with visits data is well used by product managers to understand their ‘live’ users but it has utility outside this application. I was able to use it to better understand the lifetime of “online events” like marketing campaigns. The typical analysis of marketing campaign success is to aggregate all the views of campaign pages and their linked purchases to calculate a single, overall conversion rate for it. However, I found that changing the perspective of a single conversion rate to time series of conversion proability brings deeper understanding of conversion of campaign lifetimes and whether they influence users to purchase quickly or mull over their decision over some period of time. While this can be conflated with set times for campaigns, it is a useful tool for campaigns that last some period of time rather than one day sales type campaigns."
  },
  {
    "objectID": "projects/bayesian-size-models/index.html",
    "href": "projects/bayesian-size-models/index.html",
    "title": "Learning customer preference: from size to fit",
    "section": "",
    "text": "The conundrum of size vs. fit continues to plague online shoppers and in turn, retailers. The iteration of purchase, retention / return and re-purchse is an opportunity for retailers to learn customer preferences and offer better purchasing advice for future customers. With only transaction and browsing behaviour data, recommendations are a popular offering. However, if retailers can use customer shape data directly, obtained through online fit tools, they can evolve their understanding of size as fit as a pathway for better customer advice.\nIn this project, I chose to remodel the size chart as a “fit chart”. A fit chart begins as a size chart - one that is usually designed by the product team at the retailer / brand. Once the garment is on the site and has associated purchases or returns, the size chart can be re-calculated as a fit chart through a suitable statistical model.\nGiven the typical distributions of population parameters, I began with a bayesian model that modelled each fit point (bust, waist and hips) with a simple normal distribution. While the model performance is highly dependent on the retailer and customer behaviour combinations, I found that some retailer models could attain AUC values of 0.78 through this simple shift in perspective from size to fit. Aside from its simplicity, the bayesian model is also able to handle the large discrepancy in sales between ‘popular’ and ‘select’ items and able to offer useful outputs with both low and large volumes of sales data.\n\nExtending the simple normal model to including priors was able to significantly improve the fit charts. While the choice of a suitable prior was challenging I found that the impact of a good prior resulted in tidier outputs from the model. Even a prior that is an average across all the data can make a difference. For example, the dispersion and widths of the distributions can be reduced significantly.\n\nFeatured image Yoda by Ben Davis from the Noun Project"
  },
  {
    "objectID": "projects/attribution-experiment/index.html",
    "href": "projects/attribution-experiment/index.html",
    "title": "Natural experiments for online behaviours",
    "section": "",
    "text": "A common problem for websites and web-based technologies is understanding the impact of changes on user behaviour. The Randomised Control Trial (RCT), also referred to as A/B testing, is the gold standard framework for estimating impact. However, setting up A/B tests can be challenging for all types of changes or for embedded and third party services.\nOne avenue for learning about impact is adapting channel attribution methodologies from marketing. Attribution analysis was designed for multimedia advertising where the television or paper ad cannot be easily exposed as variants for the viewers. The power of the analysis comes instead from the acknowledgement that exposure impact can be measured from pathways to the desired outcome. For example, if the newspaper ad better connected to a final outcome of buying tickets to a movie, it would have a higher attribution. Of course, the key purpose of this approach was to attribute revenue to the different advertising approaches.\nReducing pathways to a desired outcome simplifies the problem of impact analysis to the effect of an individual element on the final goal. This perspective works well for web browsing networks. I was able to show that the impact of new items or items with “enhanced features” could be seen by calculating their contribution to the final goal - which is typically purchase on the site. While this methodology is as robust as causal methodologies, it is a valuable tool to gain useful insight from a natural, real-world experiment."
  },
  {
    "objectID": "posts/fuel-station-2/index.html",
    "href": "posts/fuel-station-2/index.html",
    "title": "Geospatial proximity analysis with fuel stations - networks",
    "section": "",
    "text": "In the previous post, we obtained and plotted locations of Z and BP stations in Wellington, New Zealand. We could see some differences in the coverage of Z vs. BP but there was no articulation of these differences. In this post, we’ll use network analysis to generate a structural picture of the two fuel station networks. We’ll also compare the two brands with commonly used network metrics.\nWe build up the abstract network in 4 steps:\nWe can think of the abstract network of a different data structure and this format allows for some novel characterisation of the fuel station coverage. The ones considered here are:\nThese metrics quantify the interaction between Z and BP fuel stations and they help build a picture of coverage."
  },
  {
    "objectID": "posts/fuel-station-2/index.html#introduction-to-street-network-analysis",
    "href": "posts/fuel-station-2/index.html#introduction-to-street-network-analysis",
    "title": "Geospatial proximity analysis with fuel stations - networks",
    "section": "Introduction to street network analysis",
    "text": "Introduction to street network analysis\nTo construct the abstract network, we first need to calculate the best route (and its distance) between every pair of fuel stations in the network. With the OSMnx (a portmanteau acronym of Open Street Maps, OSM, and NetworkX, nx) package, we can superimpose entities with geolocation on the spatial network. Once we’ve done this, we can find a path (route) connecting any two nodes. Because of the representation constraints, we don’t find the route between the 2 specific entity coordinates (like Google Maps) - instead, we find the path between two nodes closest to the entities.\nThe underlying representation used by OSMnx is a reduction of streets and roads to edges with intersectionsas the vertices (or, nodes). This representation is also known as a ‘Primal Graph’. The position of the nodes and the trajectory of the edges are further described with geolocation coordinates. The technical aspects are presented in this paper by Geoff Boening: the author of OSMnx.\nThe route between the nodes uses the edges (streets and roads) of the spatial network. The route calculation algorithm is an analogue of the typical shortest path analyses done in network science. In our spatial network, the path length can be equated to distance. The base unit of length is metres.\n\nSimple example: route between Z Kilbirnie and Z Vivian St\nThe following example looks at the distance and route between two Z stations: Z Kilbernie and Z Vivian St. The red line in the figure is the shortest route that connects the two stations. From the street shapes, you can see that the route is wending it’s way around Evans Bay and Basin Reserve, before entering the central city street grid. This route has a distance of 4.6 km - a value that corresponds quite closely to that given by Google Maps.\n\n\n\npng\n\n\nShortest distance between Z Kilbirnie and Z Vivian St is 4577.443 m\n\n\nGet street network for Wellington\nIn the toy example, I only downloaded the street network within a 5km radius of Z Kilbirnie. For the main analysis though, we want all the streets and roads within the defined bounding box. The updated figure now shows the route between Z Kilbirnie and Z Vivian St overlayed on all the roads in the Wellington + Lower Hutt bounding box.\n\n\n\npng\n\n\nShortest distance between Z Kilbirnie and Z Vivian St is 4268.96 m"
  },
  {
    "objectID": "posts/fuel-station-2/index.html#abstract-networks",
    "href": "posts/fuel-station-2/index.html#abstract-networks",
    "title": "Geospatial proximity analysis with fuel stations - networks",
    "section": "Abstract networks",
    "text": "Abstract networks\nTo construct the abstract network, we first calculate the route and distance between all possible pairs of the 13 Z stations in the region. The following table shows a subset of the results. We see distances from several Z stations to Z Broadway (in Strathmore).\n\n\n\ndistance\nid_from\nfrom\nid_to\nto\n1\n2\n3\n4\n5\n\n\n\n\n2822.644\n3120151445\nZ Kilbirnie\n5821475056\nZ Broadway\n\n\n\n\n\n\n\n1332.762\n5821475059\nZ Miramar\n5821475056\nZ Broadway\n\n\n\n\n\n\n\n4002.103\n5821475061\nZ Constable Street\n5821475056\nZ Broadway\n\n\n\n\n\n\n\n5644.063\n5821475058\nZ Taranaki Street\n5821475056\nZ Broadway\n\n\n\n\n\n\n\n5744.885\n5544110098\nZ Vivian St\n5821475056\nZ Broadway\n\n\n\n\n\n\n\n\nThe table of pairwise distances can now be used to analyse the number of neighbours for a Z/BP station within a particular radius and recast into a network structure. The recasting is useful since we can use some standard network analysis tools available in the networkx package.\nThe steps of the recasting are:\n\nFilter the pairiwse distance table to only include separations less than or equal to 10km. This step would remove the connection between Z Broadway and Z Petone for example.\nStore the filtered distance table as a network data structure. This means:\n\nStations become nodes\nAny node within 10km becomes a connecting edge\nThe distance between the nodes is stored as a weight - with shorter distances having a higher ‘weight’\n\nRemove the geolocation information for the nodes\n\nWe can visualise the network structure of the simpler, recast data. The weighted edges come in useful since closer nodes have thicker edges and are closer together than nodes that are further away. Some interesting insights include: - 2 components are apparent in the Z station network for Wellington: Wellington City and Lower Hutt. - The Z Wellington City component is very tightly connected for the central and southern suburbs. - The connectivity of the Z Wellington City component reduces for the northern stations. The table of connections shows that stations in the southern suburbs are connected to two more stations than the northern suburbs and Lower Hutt. - BP stations in the northern suburbs are a little better connected. - The BP Wellington component is not as well connected as the Z station Wellington City component. - The BP Lower Hutt component is much better connected than Z. BP also has 2 more stations in Lower Hutt compared to Z.\n\n\n\n\n\n\n\nZ network\nBP network\n\n\n\n\n\n\n\n\n\nAll these points indicate that while Z and BP cover similar areas of Wellington, Z is better represented in Wellington City while BP dominates in Lower Hutt. It would be very interesting to see if revenue per station is signficantly different for a Z station in Wellington City vs. Lower Hutt.\n\nZ Network analysis: degree\nBecause the Z station network was a loosely connected network with two strongly connected components, we can calculate the average degree per component, without much loss of accuracy. The BP network doesn’t have the same structure - BP stations are reasonably well connected throughout the Wellington region network - so we can’t analyse the BP components separately..\nThe explicit connectivity of each Z station is given by a metric called ‘degree’ in network analysis. The degree distribution is useful for understanding characteristics of structure in larger &/ complex networks. Here, it’s simply useful to use the node degree to understand the highly connected / central Z stations. As expected, these stations are the ones in the city centre: Z Harboour City, Z Vivian St, Z Taranaki Street.\n\n\n\nstation\ndegree\n\n\n\n\nZ Harbour City\n8\n\n\nZ Vivian St\n8\n\n\nZ Taranaki Street\n8\n\n\nZ Constable Street\n7\n\n\nZ Miramar\n6\n\n\nZ Broadway\n6\n\n\nZ Kilbirnie\n6\n\n\nZ Crofton Downs\n5\n\n\nZ Petone\n5\n\n\nZ Johnsonville\n5\n\n\nZ Hutt Road\n4\n\n\nZ High Street\n4\n\n\nZ VIC Corner\n4\n\n\nZ Seaview\n4\n\n\n\nThe average degree / connectivity for the Wellington City Z stations is much higher than Lower Hutt. The typical Z station in Wellington City is connected to 2 more Z stations than a typicsl Z station in Lower Hutt.\n\nZ stations in Wellington region have an average of 5.71 neighbours\nZ stations in Wellington City have an average of 6.5 neighbours\nZ stations in Lower Hutt have an average of 4.67 neighbours"
  },
  {
    "objectID": "posts/fuel-station-2/index.html#nearest-neighbours-inter-station-separation",
    "href": "posts/fuel-station-2/index.html#nearest-neighbours-inter-station-separation",
    "title": "Geospatial proximity analysis with fuel stations - networks",
    "section": "Nearest neighbours: Inter-station separation",
    "text": "Nearest neighbours: Inter-station separation\nWe can further reduce the pairwise distance matrix to the closest neighbour per statiion to calculate the average distance between any two Z stations. A more academic name for this metric is: average inter-station separation.\n\n\n\npng\n\n\nThe plots show that the physical coverage of Z vs. BP stations using the inter-station separation distances is asymmetric. There is also some indication of a bimodal distribution: a cluster of stations that are very close together and another cluster that are further apart. The difference between the two modes seems to be larger for BP.\nFrom this comparison, we can say that Z stations are better spread in the Wellington region compared to BP. We need to exercise some caution however; with only ~13 stations, we don’t have much sample size. If we do a more complete analysis for Z, we can get robust statistics by running a hierarchical model for the average inter-station separation across the different types of regions. Until then, we just have to be mindful of how strongly we present this message.\nAverage (mean) inter-station distances: - Z stations in Wellington are 2.412 km apart on average - BP stations in Wellington are 3.125 km apart on average"
  },
  {
    "objectID": "posts/fuel-station-2/index.html#nearest-neighbours-same-brand-competitor",
    "href": "posts/fuel-station-2/index.html#nearest-neighbours-same-brand-competitor",
    "title": "Geospatial proximity analysis with fuel stations - networks",
    "section": "Nearest neighbours: same brand / competitor?",
    "text": "Nearest neighbours: same brand / competitor?\nA key characteristic of good coverage is location in relation to other entities - especially competitors. A simplistic view of good coverage is that a franchise should ideally be placed close to one of its own rather than near a competitor. Some comparative analyses that explore the type of nearest neighbour include:\n\nSeeing whether Z stations neighbour each other or a competitor\nWhich Z stations are in a zone of poaching risk - i.e. their customers might go to a nearby competitor.\n\nFor this analysis, we need to generate the shortest distances between all station pairs for both Z and BP stations together. Unfortunately, the computation is not fast and needs to better managed in the future for a larger dataset. Also, note that there is an implicit redundancy in the numbers cited below: some station pairs are each others nearest neighbours.\n\n\n\nfrom\nto\ndistance\nfrom_brand\nto_brand\n\n\n\n\nBP Melling\nZ VIC Corner\n158.905\nBP\nZ\n\n\nZ Johnsonville\nBP Johnsonville\n165.593\nZ\nBP\n\n\nBP Johnsonville\nZ Johnsonville\n165.593\nBP\nZ\n\n\nZ VIC Corner\nBP Melling\n185.207\nZ\nBP\n\n\nZ Vivian St\nZ Taranaki Street\n436.455\nZ\nZ\n\n\n\n\nOut of 28 Z and BP stations, 19 are closest to a competitor and 9 are next to one of their own\nOf the 9 stations next to their own, 8 are from Z\nOf the 19 stations next to a competitor, 10 are from Z\n\n\n\n\nfrom\nto\ndistance\nfrom_brand\nto_brand\n\n\n\n\nZ Vivian St\nZ Taranaki Street\n436.455\nZ\nZ\n\n\nZ Taranaki Street\nZ Vivian St\n438.123\nZ\nZ\n\n\nZ Miramar\nZ Broadway\n1332.762\nZ\nZ\n\n\nZ Broadway\nZ Miramar\n1332.762\nZ\nZ\n\n\nZ Kilbirnie\nZ Miramar\n2111.346\nZ\nZ\n\n\nZ Petone\nZ Hutt Road\n2227.071\nZ\nZ\n\n\nZ High Street\nZ VIC Corner\n2356.524\nZ\nZ\n\n\nZ Crofton Downs\nZ Harbour City\n5270.263\nZ\nZ\n\n\nBP Wainuiomata\nBP Waiwhetu\n6335.055\nBP\nBP\n\n\n\nZ stations with a BP station within the average station-station separation distance are at risk of having their users poached by the competitor. The list below shows that key poaching zones are: Johnsonville, Western / Central Hutt, Seaview, central Wellington and south-central Wellington (Newtown, Berhampore).\n\n\n\nfrom\nto\ndistance\nfrom_brand\nto_brand\n\n\n\n\nBP Melling\nZ VIC Corner\n158.905\nBP\nZ\n\n\nZ Johnsonville\nBP Johnsonville\n165.593\nZ\nBP\n\n\nBP Johnsonville\nZ Johnsonville\n165.593\nBP\nZ\n\n\nZ VIC Corner\nBP Melling\n185.207\nZ\nBP\n\n\nBP Roadmaster\nZ Taranaki Street\n729.384\nBP\nZ\n\n\nBP Seaview Truckstop\nZ Seaview\n731.884\nBP\nZ\n\n\nZ Seaview\nBP Seaview Truckstop\n748.029\nZ\nBP\n\n\nBP Adelaide Road\nZ Taranaki Street\n1003.535\nBP\nZ\n\n\nZ Harbour City\nBP Roadmaster\n1162.016\nZ\nBP\n\n\nBP Berhampore\nZ Constable Street\n1351.750\nBP\nZ"
  },
  {
    "objectID": "posts/fuel-station-2/index.html#what-next",
    "href": "posts/fuel-station-2/index.html#what-next",
    "title": "Geospatial proximity analysis with fuel stations - networks",
    "section": "What next?",
    "text": "What next?\nFor the conclusion of which fuel station covers Wellington better, go to final instalment of the series or you can revisit the introductory post."
  },
  {
    "objectID": "posts/fuel-station-3/index.html",
    "href": "posts/fuel-station-3/index.html",
    "title": "Geospatial proximity analysis with fuel stations - accessibility",
    "section": "",
    "text": "The previous two parts Part 1 and Part 2 retrieved open spatial data for plotting onto a map from OpenStreetMap; and abstracted spatial networks to networks for connectivity analyses."
  },
  {
    "objectID": "posts/fuel-station-3/index.html#accessibility-analysis",
    "href": "posts/fuel-station-3/index.html#accessibility-analysis",
    "title": "Geospatial proximity analysis with fuel stations - accessibility",
    "section": "Accessibility analysis",
    "text": "Accessibility analysis\nThe previous analyses have only considered the fuel stations and quantify implicit interactions (via road distance) between them. But we get the real benefit of spatial analyses when we consider interactions between the fuel stations and other entities - from humans to other businesses.\nOne type of interaction with general entities is accessibility. Simple accessibility analyses convert the base geography of the region into a point grid, and compute distances between every point and the POIS.\nAccessibility is a core analysis in urban planning. Some examples here. There are even tools which score regions with scores based on accessibility e.g. WalkScore.\n\nCalculating accessibility\n\n\n\npng\n\n\nHere, we consider accessibility as the driving distance in meters from each grid point (also referred to as nodes) to the nearest POIS: a fuel station. For a visual acessibility analysis we need to: - Break up the street map into a grid of points (II) - Calculate the distance from each point (within some radius) to the nearest POIS (III) - Visualise distance as a heatmap (IV)\nAll the above steps are carried out by the Python package Pandana. The second step has a few associated sub-steps. These are: - Download OSM data within the specified bounding box - Convert street map to a point grid. - Store points data in a convenient data structure: a Pandas dataframe\n\n\nAccessibility to Z vs. BP\n\n\n\n\n\n\n\nZ\nBP\n\n\n\n\n\n\n\n\n\nThe accessibility heatmaps indicate that both Z and BP have reasonable coverage in the Wellington region. Most of the suburbs seem to be within reasonable (5km) driving distance of a Z / BP fuel station. The heatmaps also highlight some fine-grained details:\n\nZ has no coverage in Wanuiomata while BP does.\nZ doesn’t cover Karori.\nBP doesn’t cover the Southern suburbs - specifically those in the Miramar Peninsula (Miramar, Strathmore, Seatoun). Z has excellent coverage in this region.\nNeither Z nor BP cover Eastbourne.\nZ coverage in Lower Hutt is mainly along the main artery of the Hutt: Hutt Road + High Street.\nBP covers Eastern Hutt better in addition to covering along the artery line.\n\nThe northern peripheries of the map (e.g. Taita north on the Lower Hutt side and Churton Park north on the Porirua side are not be considered in this analysis as we’re missing the complete fuel station data for these areas.\nThe points made in the above section can be seen more clearly with differences between Z and BP accessibility. Since the accessibility analysis uses the same nodes for both stations, we can calculate a differential value of BP accessibility from Z acessibility: Z accessibibiity - BP accessibility. In other words, &gt; For any given node, what is the differential distance to a BP station compared to a Z station?\nThis analysis shows the interplay between Z and BP accessibility within the Wellington region, with accessibility now spanning both positive and negative values. The heatmap colours map positive values to nodes where Z stations are further away than BP; negative colours to nodes where Z stations are closer than BP.\nA summary of the differential heatmap colours:\n\nRed and organge regions indicate better BP coverage (i.e. BP station is closer).\nBlue and green indicate better Z coverage (i.e. Z station is closer).\nYellow tones indicate good coverage for both stations (i.e. equivalent distance to both Z and BP station).\n\n\n\n\npng\n\n\nThe convenience of the common accessibility nodes can help us approximate an average accessibility to a Z / BP station for the Wellington region. The histograms plot the distance to the nearest Z / BP station. The mean and median show two quite interesting points: - There are more nodes that are more than 5km from a Z station. These “inaccessible” nodes increase the mean accessibility to a Z station for the region. The difference in the mean accessibility of Z and BP are fairly close. Z stations are only closer by 100 m. - If the inaccessible nodes are not considered as strongly (i.e. using the median), Z stations are more accessible by 200m.\nWhile Z does better for the covered regions, the regions that aren’t covered by Z reduce the average accessibility.\n\n\n\npng\n\n\nWe can explicitly only plot the nodes that fall within the accessible areas. Z accessibility is 200 m better than BP for both mean and median averages. The previous plot also showed some indication of a bimodal distribution in accessibility for BP. Increasing the number of bins shows that there is likely a bimodal accessibility distribution for BP (similar result in an earlier section). But, we’d need to think a little deeper as to why this might be the case.\n\n\n\npng"
  },
  {
    "objectID": "posts/fuel-station-3/index.html#conclusions",
    "href": "posts/fuel-station-3/index.html#conclusions",
    "title": "Geospatial proximity analysis with fuel stations - accessibility",
    "section": "Conclusions",
    "text": "Conclusions\nThe key business question to be answered in this series was: &gt; Does Z have better coverage than their competitor(s) in Wellington? If so, how?\nThrough a variety of different spatial analyses, we can quantify some aspects of this question:\n\nZ stations are closer together on average, compared to BP.\nZ stations in Wellington City are closely-knit. A Z station in Wellington City has 6.5 Z stations within 10 km. That’s 2 more stations than the average for the Lower Hutt sub-network (component).\n\nBP have more physical stations in Lower Hutt.\nAverage accessibility to Z stations is better than BP stations.\nBoth Z and BP vie for coverage in the Wellington region - doing well in some areas and poorly in others:\n\nZ has exclusive coverage in the Miramar Peninsula and North Western suburbs (Ngaio, Crofton Downs). Petone and the Hutt along the main Hutt Road / High Street artery of Lower Hutt are also better served by Z.\nBP exclusively covers Wainuiomata and Karori. Parts of the Southern suburbs (like Island Bay) are also better covered by BP.\n\n\nGiven the above findings, we can tentatively answer the business question &gt; Z stations have better coverage in Wellington compared to BP.\nHowever, this tentative conclusion needs more work to become a substantive one. Suggestions in the following section."
  },
  {
    "objectID": "posts/fuel-station-3/index.html#future-work",
    "href": "posts/fuel-station-3/index.html#future-work",
    "title": "Geospatial proximity analysis with fuel stations - accessibility",
    "section": "Future Work",
    "text": "Future Work\nSome immediate follow up work can include:\n\nA more complete analysis. Since Z is not the only brand of the Z entity, we need to include Caltex Stations as well.\nCombining population density with accessibility. Currently, accessibility analysis considers every node to contain the same number of people. This is clearly untrue. A better understanding of the weaknesses in Z’s coverage would need to know which high population density nodes have poor access to Z stations."
  },
  {
    "objectID": "posts/rap-public-sector/index.html",
    "href": "posts/rap-public-sector/index.html",
    "title": "RAPping in the public sector",
    "section": "",
    "text": "Public sector, indeed even private sector, analytics are rife with silos and people-driven pipelines. Instead of building processes with minimal manual interference, pipelines are ususally a mash of the metaphorical ductape and frenetic manual steps resulting in blood, sweat and tears for any analyst who subsequently picks up the work.\nManual processes need to be overseen: to start, pause, stop, make changes, perform checks etc. This overload of analyst headspace can have costly outcomes due to inevitable human error. Fortunately, most of these steps can be automated allowing the analyst to dedicate their skills to using the data and providing business-relevant value.\n\n\n\n\ngraph TD\ndata --&gt; a[Put in Excel]\na --&gt; b[Run manual calculations]\nb --&gt; c1[Excel sheet &lt;br&gt; with &lt;br&gt; manual checks]\nb --&gt; c[Copy output &lt;br&gt; to &lt;br&gt; Excel / Word]\nc --&gt; e[Review]\ne --&gt; |Changes needed| b\ne  --&gt; |Changes needed| data"
  },
  {
    "objectID": "posts/rap-public-sector/index.html#the-ubiquity-of-bad-processes",
    "href": "posts/rap-public-sector/index.html#the-ubiquity-of-bad-processes",
    "title": "RAPping in the public sector",
    "section": "",
    "text": "Public sector, indeed even private sector, analytics are rife with silos and people-driven pipelines. Instead of building processes with minimal manual interference, pipelines are ususally a mash of the metaphorical ductape and frenetic manual steps resulting in blood, sweat and tears for any analyst who subsequently picks up the work.\nManual processes need to be overseen: to start, pause, stop, make changes, perform checks etc. This overload of analyst headspace can have costly outcomes due to inevitable human error. Fortunately, most of these steps can be automated allowing the analyst to dedicate their skills to using the data and providing business-relevant value.\n\n\n\n\ngraph TD\ndata --&gt; a[Put in Excel]\na --&gt; b[Run manual calculations]\nb --&gt; c1[Excel sheet &lt;br&gt; with &lt;br&gt; manual checks]\nb --&gt; c[Copy output &lt;br&gt; to &lt;br&gt; Excel / Word]\nc --&gt; e[Review]\ne --&gt; |Changes needed| b\ne  --&gt; |Changes needed| data"
  },
  {
    "objectID": "posts/rap-public-sector/index.html#rap-to-overcome-bad-processes",
    "href": "posts/rap-public-sector/index.html#rap-to-overcome-bad-processes",
    "title": "RAPping in the public sector",
    "section": "RAP to overcome bad processes",
    "text": "RAP to overcome bad processes\nOne nifty framework for moving from manual processes is RAP or reproducible analytical pipelines. Coined by the UK Government Statistical Service, RAP brings in concepts and practices from data engineering, devops and software carpentry domains to analysts in the public sector.\n\nReproducible Analytical Pipelines (RAPs) are automated statistical and analytical processes. They incorporate elements of software engineering best practice to ensure that the pipelines are reproducible, auditable, efficient, and high quality.\n\nThese practices include:\n\nSubstituting manual steps with code\nUsing modern, open source programming languages\nConverting raw data to statistical output with pipelines / workflows\nUsing version control to keep records of development\nBringing in code review practices\n\nDespite its utility, RAP focuses primarily on converting data from a commonly-managed data store into analytical outputs (reports, tables, models etc). However, in the infrastructure-poor environments of many public sector organisations, data is often inaccessible with no automated process that transforms it from raw data to a form fit for subsequent RAPping. This means the concepts of RAP need to be brought further back into the data analysis process - into the ‘data engineering domain’.\n\n\n\n\n\ngraph LR\nrd[Raw data source] --&gt; lc[Local copy]\nlc --&gt; pr1[Transformation application]\npr1 --&gt; pd[Processed data]\npd --&gt; rep[Reports]\npd --&gt; tab[Tables]\npd --&gt; db[Dashboards]"
  },
  {
    "objectID": "posts/rap-public-sector/index.html#rapping-with-legacy-code",
    "href": "posts/rap-public-sector/index.html#rapping-with-legacy-code",
    "title": "RAPping in the public sector",
    "section": "RAPping with legacy code",
    "text": "RAPping with legacy code\nFor any analyst who has inherited a pre-existent data processing code base replete with manual management, it’s not trivial to rewrite it with RAP principles. Complicated functionality can be difficult to rewrite, and some data extraction steps are challenging. One example of a hard-to-move legacy code base has the following steps:\n\n\n\n\nflowchart LR\n    \n    subgraph EXTRACT\n        direction TB\n            oi[Outlook inbox] --&gt; |Download manually|gs[Get and save data]\n            gs --&gt; |Rename file &lt;br&gt; change column types|rd[Raw data store]\n    end\n    \n    subgraph TRANSFORM\n        direction TB\n            e[Raw data store] --&gt;|Manual filename change| sas_read\n            sas_read[Read raw data &lt;br&gt; into SAS format] --&gt; |Manual filename change| sas_trans[Transform data]\n            sas_trans --&gt; data[Processed data]\n    end\n\nfa[feed A] --&gt;|automated &lt;br&gt; delivery| EXTRACT --&gt; TRANSFORM \n\n\n\n\n\nIn the following sections, I will briefly sketch one route to RAPping this difficult process. Subsequent posts will cover more nuance in creating local versions of RAP (that include data engineering) and different strategies for making the best use of open source tools and practices. Note, much of the following content can be found in my talk at Statistics New Zealand.\n\nPython as glue\nPython is a modern, multi-paradigm, evolving, open source programming language. It is used widely across many domains - from web development to data science. Due to its breadth of use and popularity, there is an incredible ecosystem of packages. In addition, manual steps like the following can all be done using Python.\n\nChanging file names in scripts to the latest data\nGetting data deliveries from Outlook inboxes\nRunning scripts in order\n\nPython and its rich ecosystem of packages can be used be used as a glue, or interface between different programs. Packages like exhangelib and saspy can connect to APIs and programs like Outlook and SAS respectively. More on how these packages facilitate automation in the following sections.\nAnother aspect of pythonic glue is gluing together a linear pipeline / workflow in a Jupyter notebook. Cells in the notebook can be run in any order manually but using the Run All command sets up a linear execution - cells are run in series giving immediate linear dependency.\n\n\n\n\ngraph TD\nf[feed A] --&gt; e[Outlook inbox]\nbeq --&gt; |exchangelib| e\nsc[SAS Code] --&gt; |yaml| pws\n\nsubgraph Jupyter NOTEBOOK\n    dr[Date range] --&gt; beq[[Build email query]]\n    beq --&gt; |exchangelib &lt;br&gt; pandas|gsd[[Get and save data]]\n    gsd --&gt; rd[Raw data store]\n    rd --&gt; |saspy| pws[[Process with SAS]]\n    pws --&gt; pd[Processed data]\nend\n\n\n\n\n\nJupyter notebooks also have additional features like:\n\nIncluding documentation alongside code execution - easily updated while pipeline development is still in flux\nIncluding checks (as tables or graphs) as part of the pipeline making the executed notebook a log of the processing run that can be saved for posterity.\nUsing Python’s try execpt can be used to raise errors and stop execution of the pipeline to give the analyst time to correct.\nSince executed outputs are stored in memory so with a sensible structure, it can be quite easy to re-run the notebook from an intermediate point rather than run the entire process again after correcting any issues."
  },
  {
    "objectID": "posts/rap-public-sector/index.html#down-in-the-details",
    "href": "posts/rap-public-sector/index.html#down-in-the-details",
    "title": "RAPping in the public sector",
    "section": "Down in the details",
    "text": "Down in the details\nThe following sections give a little more detail into how exchangelib and saspy help with hard to automate tasks like:\n\nautomatically downloading relevant data sent by email\nautomating SAS code that needs to be run with manual changes (e.g. new file names, data ranges etc.)\n\n\nGetting data with APIs\nUsing email inboxes as a primary data receiver is a common problem since public sector analysts lack the technology infrastructure to transfer data between different organisations. Email ends up being a “solution”. However, using emails for frequent data feeds / frequent processing is not a sustainable process.\nApplication programming interfaces (APIs) allows applications to communicate with each other. The Outlook email program has a rich API behind it called Exchange Web Services (EWS). Applications (like our RAP extract data application) can send EWS queries to push or pull data to Outlook objects like emails, contacts and calendars. The Get and save data functionality can now store the raw dataset as well apply any required transformations - like changing the filenames or data formats.\n\n\n\n\ngraph TD\nf[feed A] --&gt; e[Outlook inbox]\nbeq --&gt; |exchangelib| e\nsubgraph EXTRACT    \n    d[Date range] --&gt; beq[[Build email query]]\n    beq --&gt; |exchangelib &lt;br&gt; pandas|gsd[[Get and save data]]\n    gsd --&gt; rd[Raw data]\nend\n\n\n\n\n\n\n\nRunning SAS through Python with SASPy\nWhen the SAS codebase is complex, large or both, it’s convenient to instead just make it run without manual changes. Furthermore, this approach allows to incremental refactoring - allowing hard-to-convert code to remain in SAS while moving easier code to Python.\nIt’s worth noting that there are actually two ways of running SAS outside the SAS program: - With a SAS kernel in Jupyter* - Through SASPy\nSASPy is officially supported by SAS, and available as an open source package. The library seems to be well-maintained and well-documented.\n\nAt its core, SASPy is capable of creating a SAS session and sending code to it for execution, as well as returning the output (logs and other output) to the controlling Python script. Yet it is also a powerful generator of SAS code, which means that it offers methods, objects, and syntax for use directly in idiomatic Python that it can then automatically convert to the appropriate SAS language statements for execution. In most cases, SAS procedures or steps are mapped directly to Python methods as a one-to-one equivalent.\n\n\n\nMutating SAS code with Python\nSASPy is able to generate SAS queries from Python commands. However, running existing SAS scripts with part of it needing amendment via Python needs some additional engineering. The easiest solution so far has three main steps:\n\nBreaking up a SAS script into yaml chunks for “immutable” components\n“Mutable” components are created in Python\nThe immutable and mutable are brought together with Python’s f-strings\n\n\n\n\n\ngraph TD\nsc[SAS Code] --&gt; |yaml| pws\nsubgraph TRANSFORM\n    rd[Raw data] --&gt; |saspy| pws[[Process with SAS]]\n    pws --&gt; pd[Processed data]\nend"
  },
  {
    "objectID": "posts/network-issues/index.html",
    "href": "posts/network-issues/index.html",
    "title": "Identifying spatial network issues",
    "section": "",
    "text": "I’ve blogged many times previously about the insights enabled by spatial networks - especially those resulting from route operations. I’ve so far relied on the vast Openstreetmap (OSM) via osmnx for my analyses. However, I have had some issues importing non-OSM data into a spatial network using omsnx. so, I’ve recently been exploring spatial networks from “custom” non-OSM geospatial datasets using the excellent sfnetworks package in R.\nAs part of some side projects at work (and a general interest) I’ve wanted to work with rail network data - while available through OSM are also available in New Zealand through more “official sources” like LINZ to Kiwirail open data. Unfortuntely, I quickly hit some snags after a period of initial excitement. My main frustrations were trying to understand why I couldn’t create a route between two clearly connected points on the network. Eventually, I realised that I needed to amend my network analysis workflow to include significant pre-processing and diagnostic tools to check network connectivity.\nThis post goes through a simple diagnostic for checking network connectivity and highlights basic steps to create a connected, routeable network. The corrections I’ve applied may not be sufficient for a different network / use case but they are a great starting point."
  },
  {
    "objectID": "posts/network-issues/index.html#spatial-networks-the-challenge",
    "href": "posts/network-issues/index.html#spatial-networks-the-challenge",
    "title": "Identifying spatial network issues",
    "section": "",
    "text": "I’ve blogged many times previously about the insights enabled by spatial networks - especially those resulting from route operations. I’ve so far relied on the vast Openstreetmap (OSM) via osmnx for my analyses. However, I have had some issues importing non-OSM data into a spatial network using omsnx. so, I’ve recently been exploring spatial networks from “custom” non-OSM geospatial datasets using the excellent sfnetworks package in R.\nAs part of some side projects at work (and a general interest) I’ve wanted to work with rail network data - while available through OSM are also available in New Zealand through more “official sources” like LINZ to Kiwirail open data. Unfortuntely, I quickly hit some snags after a period of initial excitement. My main frustrations were trying to understand why I couldn’t create a route between two clearly connected points on the network. Eventually, I realised that I needed to amend my network analysis workflow to include significant pre-processing and diagnostic tools to check network connectivity.\nThis post goes through a simple diagnostic for checking network connectivity and highlights basic steps to create a connected, routeable network. The corrections I’ve applied may not be sufficient for a different network / use case but they are a great starting point."
  },
  {
    "objectID": "posts/network-issues/index.html#set-up",
    "href": "posts/network-issues/index.html#set-up",
    "title": "Identifying spatial network issues",
    "section": "Set up",
    "text": "Set up\nAll code for this post can be found on github. The renv.lock provides the package dependencies to run this project - though it is far from a parsimonious specification as I use my projects to explore. The package can be reduced considerably as there are several package requirements (e.g. ggraph, leaflet) not needed for this particular example.\nI downloaded the New Zealand rail network from the kiwirail open data hub as a geodatabase but other formats are also available. Included in the repo is a file of port locations (port_locs dataframe) from around the world though this example (and some subsequent ones) will only use a subset of New Zealand ports.\n# kiwirail data from \n# https://data-kiwirail.opendata.arcgis.com/datasets/kiwirail-track-centreline\n\n# Basic filter and transform of rail network from Kiwirail\n# only keeping tracks\nnz_rail &lt;- st_read(here::here(\"data\", \"kiwirail.gdb\")) %&gt;%\n  filter(!Type %in% c(\"Crossover\", \"Yard Track\")) %&gt;%\n  st_transform(2193) %&gt;% \n  st_cast(\"LINESTRING\")"
  },
  {
    "objectID": "posts/network-issues/index.html#creating-a-routeable-rail-network",
    "href": "posts/network-issues/index.html#creating-a-routeable-rail-network",
    "title": "Identifying spatial network issues",
    "section": "Creating a routeable rail network",
    "text": "Creating a routeable rail network\nThe excellent new sfnetworks package offers a very simple way to create a routeable network, a graph structure, from any spatial points or lines dataset. Note, the railway network, originally a multi-linestring spatial lines dataset has to be cast as a simpler linestring for the conversion to an sfnetwork data format. This is done at the outset - when the dataset is imported.\nrailway_net &lt;- nz_rail %&gt;%\n  as_sfnetwork(directed=F)"
  },
  {
    "objectID": "posts/network-issues/index.html#associating-external-data-with-network-nodes",
    "href": "posts/network-issues/index.html#associating-external-data-with-network-nodes",
    "title": "Identifying spatial network issues",
    "section": "Associating external data with network nodes",
    "text": "Associating external data with network nodes\nIn many cases of routing problems, we’re interested in the route to a point of interest (POI) that is defined separately. Here, the POIs are port locations. For routing to be possible, these POIs need to be associated to a node in the sfnetwork graph. For the inclusion of new The new st_nearest_feature is a helper function in the sfnetworks package that finds the index of the closest feature (nodes or edges depending on choice) to the POI. In this example, we are trying to find a route from Auckland to Wellington, New Zealand.\n# Isolate nodes as an sf dataframe for ease of use\nnodes_rail &lt;- railway_net %&gt;% activate(\"nodes\") %&gt;% st_as_sf()\n\n# Specify origin and destination\nfrom = \"Auckland\"\nto = \"Wellington\"\n\n# Tibble of origin and destination and index of closest node\norig_dest &lt;- bind_cols(tibble(from_port = port_locs %&gt;% \n                                          filter(port_name == from) %&gt;% \n                                          pull(geometry)), \n                       tibble(to_port = port_locs %&gt;% \n                                        filter(port_name == to) %&gt;% \n                                        pull(geometry))) %&gt;%\n  st_as_sf(crs = 2193) %&gt;%\n  mutate(to_index = st_nearest_feature(to_port, nodes_rail), \n         from_index = st_nearest_feature(from_port, nodes_rail),\n         route = row_number())"
  },
  {
    "objectID": "posts/network-issues/index.html#routing",
    "href": "posts/network-issues/index.html#routing",
    "title": "Identifying spatial network issues",
    "section": "Routing",
    "text": "Routing\nOnce the indices of the POIs are found, we can use the st_network_paths wrapper function to find the shortest path between a single point of origin and a vector of destinations. However, the function returns no path as seen by the empty list in the edge_paths column.\n# returns function that includes the global sfnetwork object\n# afaik the map and pmap functions can only refer to columns\n# within the tibble when used inside mutate\nst_network_paths_mod &lt;- function(from, to){\n  return(try(st_network_paths(railway_net, from, to)))\n}\n\n# Look at the expanded tibble of edge_paths and node_paths returned from \n# shortest path calculation\nroutes_df &lt;- orig_dest %&gt;%\n  mutate(path = pmap(list(from = from_index, to = to_index), .f=st_network_paths_mod)) %&gt;% \n  unnest(cols=c(path)) %&gt;% \n  unnest(cols=c(node_paths, edge_paths)) %&gt;% \n  select(-from_port, -to_port)\n\nroutes_df\n## Simple feature collection with 0 features and 5 fields\n## Bounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\n## CRS:           EPSG:2193\n## # A tibble: 0 x 6\n## # … with 6 variables: to_index &lt;int&gt;, from_index &lt;int&gt;, route &lt;int&gt;,\n## #   node_paths &lt;int&gt;, edge_paths &lt;int&gt;, from_port &lt;GEOMETRY [m]&gt;"
  },
  {
    "objectID": "posts/network-issues/index.html#diagnosing-issues",
    "href": "posts/network-issues/index.html#diagnosing-issues",
    "title": "Identifying spatial network issues",
    "section": "Diagnosing issues",
    "text": "Diagnosing issues\nThe difficulty with spatial networks is that they may appear connected to the naked eye but there are some insidious issues that manifest as a disconnected graph. A simple diagnostic is to examine the connectivity of all the nodes in the network.\nI’ve chosen the simplest connectivity metric: the node degree i.e. the number of edges connected to any node. When the degree is 1, the node is only connected to a single edge. Terminal nodes, where the tracks end, are legitimate single degree nodes. However, as the interactive graph below shows, the whole rail sfnetwork is comprised of disconnected, single connection nodes. No wonder the routing algorithm couldn’t find a path!\nopen_ended_nodes &lt;- railway_net %&gt;% \n  activate(\"nodes\") %&gt;% \n  mutate(degree = centrality_degree()) %&gt;% \n  st_as_sf() %&gt;%\n  mutate(row = row_number()) %&gt;%\n  filter(degree == 1) \n\ndisconnected_edges &lt;- railway_net %&gt;% \n  activate(\"edges\") %&gt;% \n  st_as_sf() %&gt;% \n  filter(from %in% open_ended_nodes$row | to %in% open_ended_nodes$row)\n\n\n# Visualise connectivity\nmapview(railway_net %&gt;% activate(\"edges\") %&gt;% st_as_sf(), layer.name=\"rail network\") + \n  mapview(disconnected_edges, color=\"red\", layer.name = \"nodes with only 1 edge\") + \n  mapview(open_ended_nodes, color=\"red\", col.regions=\"red\", layer.name = \"edges of 1 edge nodes\")"
  },
  {
    "objectID": "posts/network-issues/index.html#managing-the-issues",
    "href": "posts/network-issues/index.html#managing-the-issues",
    "title": "Identifying spatial network issues",
    "section": "Managing the issues",
    "text": "Managing the issues\nAfter some time purusing the excellent vignettes on the sfnetworks site and some googling, I found that there were two key issues preventing the creation of a connected network:\n\ntoo high precision of coordinates leading to small gaps between what should be connected edges and,\nedges connected to interior nodes rather than terminal nodes.\n\nThe first is a data problem - where coordinate precision is slightly off possibly due to rounding precision set by the GIS export. The degree of rounding to the nearest 10 m is likely a little high and I will be looking to decrease it in the near future. The second problem appears to be a peculiarity of the sfnetworks paradigm where edges that aren’t connected at terminal nodes are considered disconnected.\nLuckily, solving these two issues is not too challenging with sensible suggestions here for the first problem and sfnetworks documentation on the operation to_spatial_subdivision for the second. I’ve also included a to_simple_simple morpher to decrease the size of the network object which more light weight visualisations and, faster processing on the network.\n# rounding coordinates to prevent precision errors\n# which created disconnected edges in the network\nst_geometry(nz_rail) &lt;- nz_rail %&gt;% \n  st_geometry() %&gt;% \n  map(~round(., -1)) %&gt;% \n  st_sfc(crs = st_crs(nz_rail))\n\n# subdividing edges where intersections happen at \n# internal points. excluding this creates a disconnected network\n# because sfnetwork only connects edges where edges join at the \n# terminal nodes\nrailway_net &lt;- nz_rail %&gt;%\n  as_sfnetwork(directed=F) %&gt;% \n  convert(to_spatial_subdivision) %&gt;% \n  convert(to_spatial_simple)"
  },
  {
    "objectID": "posts/network-issues/index.html#a-connected-network",
    "href": "posts/network-issues/index.html#a-connected-network",
    "title": "Identifying spatial network issues",
    "section": "A connected network?",
    "text": "A connected network?\nIf we repeat the same routing calculation between the two points, we see now that there is a path! Note, the origin and destination indices need to be recalculated since the network has been transformed in a couple of ways.\n# re-extract nodes list\nnodes_rail &lt;- railway_net %&gt;% activate(\"nodes\") %&gt;% st_as_sf()\n\n# Tibble of origin and destination and index of closest node\norig_dest &lt;- bind_cols(tibble(from_port = port_locs %&gt;% \n                                          filter(port_name == from) %&gt;% \n                                          pull(geometry)), \n                       tibble(to_port = port_locs %&gt;% \n                                        filter(port_name == to) %&gt;% \n                                        pull(geometry))) %&gt;%\n  st_as_sf(crs = 2193) %&gt;%\n  mutate(to_index = st_nearest_feature(to_port, nodes_rail), \n         from_index = st_nearest_feature(from_port, nodes_rail),\n         route = row_number())\n\n# Recalculate route\nroutes_df &lt;- orig_dest %&gt;%\n  mutate(path = pmap(list(from = from_index, to = to_index), .f=st_network_paths_mod)) %&gt;% \n  unnest(cols=c(path))\n\n# Extract and enrich route with coordinates for visualisation\nauck_wlg &lt;- routes_df %&gt;% \n  unnest(edge_paths) %&gt;% \n  select(edge_paths) %&gt;% \n  st_drop_geometry() %&gt;% \n  inner_join(railway_net %&gt;% \n               activate(\"edges\") %&gt;% \n               st_as_sf() %&gt;% \n               mutate(edge = row_number()), by = c(\"edge_paths\" = \"edge\"))  %&gt;% \n  st_as_sf()\n\n# Visualise route\nggplot() + \n  geom_sf(data = railway_net %&gt;% activate(\"edges\") %&gt;% st_as_sf()) + \n  geom_sf(data=auck_wlg, colour=\"red\")"
  },
  {
    "objectID": "posts/network-issues/index.html#why-does-routing-work-now",
    "href": "posts/network-issues/index.html#why-does-routing-work-now",
    "title": "Identifying spatial network issues",
    "section": "Why does routing work now?",
    "text": "Why does routing work now?\nRepeating the calculation of the node degree, we now see that there are very few disconnected nodes in the corrected network. The nodes that are disconnected and not at the end of the tracks are actually odd little side tracks - probably around railway stations and junctions.\nopen_ended_nodes &lt;- railway_net %&gt;% \n  activate(\"nodes\") %&gt;% \n  mutate(degree = centrality_degree()) %&gt;% \n  st_as_sf() %&gt;%\n  mutate(row = row_number()) %&gt;%\n  filter(degree == 1) \n\ndisconnected_edges &lt;- railway_net %&gt;% \n  activate(\"edges\") %&gt;% \n  st_as_sf() %&gt;% \n  filter(from %in% open_ended_nodes$row | to %in% open_ended_nodes$row)\n\n# Visualise corrected network\nmapview(railway_net %&gt;% activate(\"edges\") %&gt;% st_as_sf(), layer.name=\"rail network\") + \n  mapview(disconnected_edges, color=\"red\", layer.name = \"nodes with only 1 edge\") + \n  mapview(open_ended_nodes, color=\"red\", col.regions=\"red\", layer.name = \"edges of 1 edge nodes\")"
  },
  {
    "objectID": "posts/walkability-3/index.html",
    "href": "posts/walkability-3/index.html",
    "title": "Walking in Wellington - validation and visualisation",
    "section": "",
    "text": "The OSM network has useful metadata that is consumed by pandana to calculate access to playgrounds and thus local walkbility. If these metrics are to be used for formal appraisals or evaluations, it is useful to check against commonly used routing APIs. Here, we look at Google Maps and GraphHopper but Targomo is also a great option - especially if the isochrone polygons are the desired output. In this post, isochrones will be constructed after the accesibility calculation using some nifty features of networkx."
  },
  {
    "objectID": "posts/walkability-3/index.html#introduction",
    "href": "posts/walkability-3/index.html#introduction",
    "title": "Walking in Wellington - validation and visualisation",
    "section": "",
    "text": "The OSM network has useful metadata that is consumed by pandana to calculate access to playgrounds and thus local walkbility. If these metrics are to be used for formal appraisals or evaluations, it is useful to check against commonly used routing APIs. Here, we look at Google Maps and GraphHopper but Targomo is also a great option - especially if the isochrone polygons are the desired output. In this post, isochrones will be constructed after the accesibility calculation using some nifty features of networkx."
  },
  {
    "objectID": "posts/walkability-3/index.html#validating-the-accessibility-analysis",
    "href": "posts/walkability-3/index.html#validating-the-accessibility-analysis",
    "title": "Walking in Wellington - validation and visualisation",
    "section": "Validating the accessibility analysis",
    "text": "Validating the accessibility analysis\nWe’ve see how Wellington’s topography affects travel times to playgrounds. A quick validation of the approach can be done with Google Maps. A more complex validation can be done Graphhopper.\nWe’ll be validating the route from 110 John Sims Drive to Kipling Street Play Area. I chose this particular example because of the monotonicity in gradient for the to and from journeys. Going to the park is consistently downhill and coming back is consistently uphill. None of the classic Wellington roller-coaster routes here!\nThe route used to calculate the accessibility in pandana can be visualised by extracting the nearest graph nodes to the origin and destination and using the Networkx get_shortest_path method.\nWith a little additional poking about the graph edges, we can extract the summary, in distance and time, for this route. The code for this is available in the Jupyter Notebook.\n\nRoute is 925 m long and takes 24.3 mins\n\n\n\n\npng\n\n\nA more detailed analysis of the route gives:\n\n110 John Sims Drive to Kipling Street Play Area: Street distance is 925 m. At 5km/hr, it takes 11.1 mins. Going to the park (downhill) takes 11.9 mins. Coming back from the park (uphill) takes 12.3 mins’\n\n\nWith Google Maps\n110 John Sims Drive to Kipling St Play Area in Johnsonville. - Uphill from the park: 14 minutes - Downhill to the park: 11 minutes\nWe don’t expect the OSM street data to differ much from Google (at least for a city!) and the elevation data should be identical. The key difference is likely the distance to time conversion. For this validation, we have over-estimated the downhill time and under-estimated the uphill time.\nobler’s time conversion is definitely an ambitious target - especially for families walking with young children. For a more accurate analysis, we’d need recommendations of a more sensible top speed and hill climbing speed retardation from Urban Planners.\n\n\nWith Graphhopper Routing API\nGraphhopper is a powerful engine with a fantastic routing API. It’s easy to register for a free licence and get an API key. Since I’m only using the API for validation, I can stay well within the free limit.\nResults from the API request can be visualised both on Graphhopper Maps and OpenStreetMap directions.\n\nDownhill route visualised here and here\nUphill route visualised here and here\n\nCrafting the API request is quite simple - the request parameters are detailed here. The resultant JSON can be parsed and used for further analysis.\nI haven’t quite figured out how to ingest this stream of rich data for further analysis. In theory, I’d like to be able to validate the speeds and times of segments in the route against the pandana + osmnx extract. Perhaps even reverse engineer the gradient to speed conversion.\ngraph_hopper_api_key = data_loaded['graph_hopper_api_key'][0]\ngraph_hopper_query = \"https://graphhopper.com/api/1/route?point=-41.2292,174.7922&point=-41.2253,174.7976&vehicle=foot&points_encoded=false&locale=nz&key=\" + graph_hopper_api_key\n%%bash -s \"$graph_hopper_query\"\ncurl $1\n{\"hints\":{\"visited_nodes.average\":\"28.0\",\"visited_nodes.sum\":\"28\"},\"info\":{\"copyrights\":[\"GraphHopper\",\"OpenStreetMap contributors\"],\"took\":4},\"paths\":[{\"distance\":1007.3,\"weight\":603.101372,\"time\":725253,\"transfers\":0,\"points_encoded\":false,\"bbox\":[174.792028,-41.229148,174.798734,-41.22499],\"points\":{\"type\":\"LineString\",\"coordinates\":[[174.792028,-41.229148],[174.792089,-41.229032],[174.792403,-41.228742],[174.792854,-41.228362],[174.792949,-41.228113],[174.792882,-41.22792],[174.793009,-41.227871],[174.793469,-41.227519],[174.79382,-41.227449],[174.79396,-41.227773],[174.794037,-41.227829],[174.794583,-41.228031],[174.795135,-41.228387],[174.79614,-41.227492],[174.797424,-41.226418],[174.797595,-41.226301],[174.79831,-41.225888],[174.79852,-41.22564],[174.798734,-41.225427],[174.798559,-41.225274],[174.798037,-41.225018],[174.797925,-41.22499],[174.797878,-41.224999],[174.797832,-41.225054],[174.797679,-41.225337],[174.797637,-41.225354]]},\"instructions\":[{\"distance\":163.218,\"heading\":21.8,\"sign\":0,\"interval\":[0,5],\"text\":\"Continue onto John Sims Drive\",\"time\":117517,\"street_name\":\"John Sims Drive\"},{\"distance\":68.299,\"sign\":2,\"interval\":[5,7],\"text\":\"Turn right onto Truscott Avenue\",\"time\":49175,\"street_name\":\"Truscott Avenue\"},{\"distance\":31.971,\"sign\":1,\"interval\":[7,8],\"text\":\"Turn slight right onto Truscott Avenue\",\"time\":23019,\"street_name\":\"Truscott Avenue\"},{\"distance\":161.515,\"sign\":2,\"interval\":[8,12],\"text\":\"Turn right onto Elliott Street\",\"time\":116290,\"street_name\":\"Elliott Street\"},{\"distance\":448.627,\"sign\":-2,\"interval\":[12,18],\"text\":\"Turn left onto Kipling Street\",\"time\":323010,\"street_name\":\"Kipling Street\"},{\"distance\":133.67,\"sign\":-2,\"interval\":[18,25],\"text\":\"Turn left\",\"time\":96242,\"street_name\":\"\"},{\"distance\":0.0,\"sign\":4,\"last_heading\":242.3439586064843,\"interval\":[25,25],\"text\":\"Arrive at destination\",\"time\":0,\"street_name\":\"\"}],\"legs\":[],\"details\":{},\"ascend\":12.986007690429688,\"descend\":66.6875,\"snapped_waypoints\":{\"type\":\"LineString\",\"coordinates\":[[174.792028,-41.229148],[174.797637,-41.225354]]}}]}"
  },
  {
    "objectID": "posts/walkability-3/index.html#inverting-accessibility-to-isochrones",
    "href": "posts/walkability-3/index.html#inverting-accessibility-to-isochrones",
    "title": "Walking in Wellington - validation and visualisation",
    "section": "Inverting accessibility to isochrones",
    "text": "Inverting accessibility to isochrones\nOur perspective so far has been the access from any street location to the nearest park. We can flip this perspective and consider the nearest street locations from the park. This perspective is the basis of isochrones.\nLike the accessibility analysis, we can choose the relevant units to visualise as isochrones. I’ve kept the total travel time units in this example.\nThe street networks below show the 0-25 minute isochrones in increments of 5 minutes. The isochrones can be visualised as convex hull polygons, or filled street skeleton isochrones. The latter visualisation has been adapted from this excellent post. The filled street skeletons offer a more realistic visualisation - especially in areas containing street networks with low connectivity like the following example.\nThe points used in the validation section are repeated here. The blue and green points represent 110 John Sims Drive and the Kipling St Play Area respectively.\n\n\n\nCatchment polygons\nCatchment along streets\n\n\n\n\n\n\n\n\n\nThe isochrones are centered on the closest street node to the park. The address we’re considering is in the 20-25 minute isochrone. This corresponds well to the total time of 25 minutes calculated in the previous section (11 minutes downhill and 14 minutes uphill).\nThe isochrones visualisation is useful but we’re limited to considering a small set of parks at a time. A more insightful analysis could be to look at residential areas that fall within the catchment zones of multiple playgrounds."
  },
  {
    "objectID": "posts/walkability-2/index.html",
    "href": "posts/walkability-2/index.html",
    "title": "Walking in Wellington - walkability metrics",
    "section": "",
    "text": "Playgrounds are important local amenities that are designed with pedestrian access in mind. Hence, accessibility to playgrounds can act as a proxy for measuring walkability of a city. Good design of local amenities should manage the various factors that might prohibit walking. For example, difficult terrain, population density induced crowding etc. In this post, we’ll be examining the following question in detail.\n\nHow prohibitive is Wellington’s topography on pedestrian accessibility to playgrounds?\n\nIn a previous post, accessibility was calculated in units of distance. Distance is an excellent metric for driving or walking on flat land. For short travels by car or walking on flat land, distance can be directly converted to travel time - since most people have an intuitive understanding of their average driving speeds (50 km/h for residential roads in New Zealand), or their approximate walking speed on flat land (usually around 5 km / h for a fit adult as given in Section 3.4 in NZTA pedestrian planning and design guide). Hills are not an issue for drivers provided road quality and safety are no different to flat land. But hills do impact travel time for pedestrians; which in turn impacts accessibility."
  },
  {
    "objectID": "posts/walkability-2/index.html#introduction",
    "href": "posts/walkability-2/index.html#introduction",
    "title": "Walking in Wellington - walkability metrics",
    "section": "",
    "text": "Playgrounds are important local amenities that are designed with pedestrian access in mind. Hence, accessibility to playgrounds can act as a proxy for measuring walkability of a city. Good design of local amenities should manage the various factors that might prohibit walking. For example, difficult terrain, population density induced crowding etc. In this post, we’ll be examining the following question in detail.\n\nHow prohibitive is Wellington’s topography on pedestrian accessibility to playgrounds?\n\nIn a previous post, accessibility was calculated in units of distance. Distance is an excellent metric for driving or walking on flat land. For short travels by car or walking on flat land, distance can be directly converted to travel time - since most people have an intuitive understanding of their average driving speeds (50 km/h for residential roads in New Zealand), or their approximate walking speed on flat land (usually around 5 km / h for a fit adult as given in Section 3.4 in NZTA pedestrian planning and design guide). Hills are not an issue for drivers provided road quality and safety are no different to flat land. But hills do impact travel time for pedestrians; which in turn impacts accessibility."
  },
  {
    "objectID": "posts/walkability-2/index.html#technical-overview",
    "href": "posts/walkability-2/index.html#technical-overview",
    "title": "Walking in Wellington - walkability metrics",
    "section": "Technical overview",
    "text": "Technical overview\nThis post is quite heavy on technical detail - accessibility analysis that includes the impact of street gradients on travel speed isn’t an out of the box analysis. So, this post is much more of a methodology post with some (hopefully, interesting and useful) insights along the way.\nKey technical challenges that needed to be overcome are listed and referenced below. Some are covered in this post, others are only available in the companion Jupyter Notebook\n\n\n\nIssue\nCovered in\n\n\n\n\nConverting accessibility metric from distance to travel time\nBlog Post\n\n\nGetting street gradients from elevation data\nBlog Post\n\n\nAdjusting travel time according to street gradients\nBlog Post\n\n\nFiltering street gradient networks\nJupyter Notebook\n\n\nCreating pandana network from osmnx graph with street gradients\nJupyter Notebook\n\n\nAdding colourbars to osmnx isochrone plots\nJupyter Notebook\n\n\nCalculating route statistics with osmnx and pandana\nJupyter Notebook"
  },
  {
    "objectID": "posts/walkability-2/index.html#data-munging",
    "href": "posts/walkability-2/index.html#data-munging",
    "title": "Walking in Wellington - walkability metrics",
    "section": "Data munging",
    "text": "Data munging\nAll the technical challenges identified above involve data munging. The relief is that we’ll only be working with two datasets, and most of the munging is done with the street network.\n\nWCC playground locations: downloaded as a zip file\nWellington street network\n\nwithout elevation: using OpenStreetMap via pandana\nwith elevation: using OpenStreetMap and Google Elevation API via osmnx\n\n\n\nWCC Playgrounds\nThe WCC playground data is easy to consume as it’s just a set of geolocations. The form is appropriate for accessibility analysis too since Points of Interest (POIs) are always single, representative coordinates (rather than polygons).\n\n\n\n\n\n\nWellington street network: without elevation\nGetting the Wellington street network in a form suitable for accessibility analysis is trivial. The previous posts on fuel station and playground accessibility cover the process in detail. Without delving into the specifics, the process simply calls pandana’s OpenStreetMap loader.\n\nChanging accessibility units\nThe default pandana network has edge weights in metres, which means that the accessibility analyses will also be in metres. We can post-hoc convert distance to travel time with an average walking speed of 5 km/h or, 83 m/minute if we want travel time in minutes.\n\n\n\nAccessibility as distance\nAccessibility as travel time\n\n\n\n\n\n\n\n\n\nThe accessibility data can be extracted and plotted as a histogram. Here, we see that the average distance to a playground is 700 m, or an 8 minute walk.\n\n\n\nDistance distribution\nTime distribution\n\n\n\n\n\n\n\n\n\n\n\n\nWellington street network: with elevation\nBoth the nodes and edges of the street network can be enriched with elevation data retrieved from Google Elevation API. Nodes have a single value for elevation. Elevation at the connecting nodes of an edge can be used to derive the inclination / gradient.\nThe above steps have been simplified to terse one-liners by the excellent Python package, osmnx, as shown in the code block below. They require signing up to the Google Elevation API and getting an API key.\nMost of the code is copied from Geoff Boeing’s tutorial. The key difference is that I’ve stored the API key in a YAML file. By the end of the code block, we have an osmnx graph with street gradients for every edge in the network.\n# Open the API keys stored in a YAML file\nwith open(\"utils/api_keys.yaml\", 'r') as stream:\n    data_loaded = yaml.load(stream)\n# Get Google Elevation API key\ngoogle_elevation_api_key = data_loaded['google_elevation_api_key'][0]\n# Create an OSMNX walking street network for the Wellington bounding box\nG = ox.graph_from_bbox(north, south, east, west, network_type='walk')\n# Add elevation values for the nodes in the OSMNX graph\nG = ox.add_node_elevations(G, api_key=google_elevation_api_key)\n# Generate an edge grade (inclination) with the elevations at the nodes\nG = ox.add_edge_grades(G)\n\nWellington street elevation profile\nosmnx generates an edge weight based on connecting node elevation values. We can extract and plot the edge gradients without and with a filter on values.\n\n\n\nAll street gradients\n“Flat” regions (within 5% gradient)\n\n\n\n\n\n\n\n\n\nThe graphs show that Wellington is largely flat around the coastline but is surrounded by hills. Larger suburbs like Karori and Johnsonville have been built on elevated plateaus.\n\n\nDealing with MultiDiGraph\nOsmnx enriches the street network with elevation and gradients using Networkx’s Multidigraph structure. In this structure, the edge (u,v) is also present as (v,u) with the opposite gradient. The duplication is why the average elevation profile is 0 and symmetric!\n\n\n\npng\n\n\nSince accessibility analysis doesn’t prescribe a starting point, we have to take both to and from journeys to fully quantify the impact of hills on travel time. To do this, we need to (1) split the components of the graph, (2) calculate accessibility for each component and, (3) sum the components as total travel time.\nThe street network can be easily split to only give unique (u,v) in one graph and the inverse, (v,u), in another. Accessibility analysis is performed for each network and the values summed together to give the total accessibility.\n\n\n\n\n\n\n\n\n\n\n\n\ngraph\ngeometry\ngrade\nlength\nname\nu\nv\n\n\n\n\nUndirected (u,v)\nLINESTRING (174.7934694 -41.2275193, 174.79300…\n0.1319\n66.800\nTruscott Avenue\n1259077823\n1259072929\n\n\nUndirected (u,v)\nLINESTRING (174.7921165 -41.2280406, 174.79263…\n-0.0475\n65.443\nTruscott Avenue\n1259077823\n1259072943\n\n\nUndirected inverse (v,u)\nLINESTRING (174.7934694 -41.2275193, 174.79300…\n-0.1319\n66.800\nTruscott Avenue\n1259072929\n1259077823\n\n\nUndirected inverse (v,u)\nLINESTRING (174.7921165 -41.2280406, 174.79263…\n0.0475\n65.443\nTruscott Avenue\n1259072943\n1259077823\n\n\n\n\n\nConverting incline distance to travel time\nA simple search led me to Naismith’s Rule and then to Tobler’s Hiking Function to calculate travel time as a function of distance and gradient.\nI’ve chosen to go with Tobler’s without much rationale other than its simple form. Tobler’s hiking function for speed, \\(\\nu\\), is a shifted exponential with three parameters: \\(a\\), \\(b\\) and \\(c\\) which give the fastest speed, speed retardation due to gradient and shift from zero respectively.\n\\[\n\\nu = a\\exp^{\\left(-b.|slope~+~c|\\right)}\n\\]\nNote that \\(slope\\) in the equation is a dimensionless quantity: \\(\\frac{dh}{dx}\\) (or, rise / run). Terminology-wise, \\(slope\\), is equivalent to gradient and inclination. Tobler’s function can also be written with slope in degrees (\\(^{\\circ}\\)). Similarly, speed is given in in km/h and can be converted to a travel time in minutes with a multiplicative factor, (60/1000). Both time and speed versions of Tobler’s function are shown in the graph below.\n\n\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\nPhysical meaning\nFastest speed\nSpeed change due to gradient\nGradient of fastest speed\n\n\nMathematical representation\n\\(\\nu_{max}\\)\n(\\(\\frac{\\Delta\\nu}{\\Delta ~gradient}\\))\n\\(gradient\\|\\nu_{max}\\)\n\n\nTobler\n6\n3.5\n0.05\n\n\nBrunsdon\n3.557\n2.03\n0.133\n\n\n\nWhile I haven’t read Tobler’s original paper, a brief exposition of other equivalent functional forms to Tobler’s has been written up by Chris Brunsdon. Brunsdon’s analysis shows a different relationship to Tobler’s - likely because the underlying data is different.\n\n\n\npng\n\n\nFor a more rigorous analysis of pedestrian accessibility, we’d refit the functional form above (or similar) as Brunsdon does for different types of pedestrians. According to NZTA and various other studies, there is significant heterogeneity in walking speed; both from the route (terrain, incline etc) and also the characteristics of the walker e.g. carrying things, footwear, and demographics. Anecdotally, we know that a commuter will walk at a very different speed to a father taking his children to the playground during the daytime."
  },
  {
    "objectID": "posts/walkability-2/index.html#accessibility-analysis-for-hilly-wellington",
    "href": "posts/walkability-2/index.html#accessibility-analysis-for-hilly-wellington",
    "title": "Walking in Wellington - walkability metrics",
    "section": "Accessibility analysis for hilly Wellington",
    "text": "Accessibility analysis for hilly Wellington\nWith a network of street gradients converted to travel time, we can now compare the impact of hills on accessibility. In the companion Jupyter notebook, there are a couple of additional, technical steps: - Creating a pandana network from an osmnx graph with travel time calculated according to the street gradient. - Calculating overall accessibility as total travel time. In the notebook this is done by calculating accessibility separately for (u,v) and (v,u) networks and summing the values.\n\nHills vs. Flat\nThe main accessibility heatmap for hilly Wellington, seen below, doesn’t look too different to the flat land assumption.\n\n\n\nFlat land assumption\nAccounting for Hills\n\n\n\n\n\n\n\n\n\nWe only see the impact of the hills when we consider a differential heatmap. Anyone familiar with the topography of Wellington will immediately note that areas around the slopes of the Town Belt and the Western Hills are affected.\n\n\n\nDifference\nDifference &gt; 2 minutes\n\n\n\n\n\n\n\n\n\n\n\nImpact of hills on travel time\nIt’s worth noting that while accessibility to playgrounds is worse due to hills, the total travel time in hilly areas only increases by 9% on average. Hilly nodes have an absolute average nearby gradient of more 5%. Sample code to calculate the average street gradient of a node is given below.\n# Extract an undirected graph from an elevation MultiDiGraph\nG_undir = G.to_undirected()\nnodes_gdfs, edges_gdfs = ox.graph_to_gdfs(G_undir)\n# Get local average gradient by node  \nu_grades = (edges_gdfs\n           .groupby('u')\n           .agg({'abs_grade': 'mean'})\n           .reset_index()\n           .rename({'u': 'id'}, axis=1))\n# Repeat for v\n...\n# Join u_grades and v_grades and drop duplicates\n...\n\n\n\nHilly Nodes\nFlat Nodes\n\n\n\n\n\n\n\n\n\n\n\nNearby options?\nThe impact of Wellington’s topography is also seen in the availability of council playground options. In a future post, it would be interesting to see the choices available per capita - since flatter suburbs are also more likely to have higher population density.\n\n\n\nNearest playground\nSecond nearest playground"
  },
  {
    "objectID": "posts/walkability-2/index.html#conclusions",
    "href": "posts/walkability-2/index.html#conclusions",
    "title": "Walking in Wellington - walkability metrics",
    "section": "Conclusions",
    "text": "Conclusions\nThe average impact of hilly terrain on playground accessibility seems to be low - only a 9% increase in total travel time. However, the average is a poor representation since the distribution of travel time is highly skewed. Even under the flat land assumption, total travel times can be higher than 40 minutes.\nIn the following post, we will compare accessibility across different residential regions of Wellington, with the aim of identifying areas with poor access."
  },
  {
    "objectID": "posts/walkability-4/index.html",
    "href": "posts/walkability-4/index.html",
    "title": "Walking in Wellington - insights from modelling",
    "section": "",
    "text": "Technical challenge\nCovered in\n\n\n\n\nGeoprocessing to get accessibility within a suburb\nJupyter Notebook\n\n\nExtracting accessibility distributions by suburb\nBlog Post\n\n\nBuild a Bayesian model for an individual suburb\nBlog Post\n\n\nModel average (\\(\\mu\\)) and heterogeneity (\\(\\sigma\\)) on two levels: (1) per suburb and, (2) across all suburbs\nBlog Post\n\n\nStan models for Bayesian analysis\nCode files\n\n\nUse average and heterogeneity, relative to Wellington average, to classify accessibility characteristic for a given suburb\nBlog Post\n\n\n\n\n\n\n\n\nDataset\nFormat\nLink\n\n\n\n\nWCC playground locations\n.zip\nWellington City Council\n\n\nWCC suburb boundaries\n.gdb\nWellington City Council\n\n\nStatsNZ Area Unit boundaries\n.gdb\nStatsNZ\n\n\nStatsNZ 2019 meshblock boundaries\n.gdb\nStats NZ\n\n\nLINZ residential polygons\n.gdb\nLINZ\n\n\nWellington street network without elevation\n-\nOpenStreetMap via osmnx\n\n\nWellington street network with elevation\n-\nOpenStreetMap + Google Elevation API via osmnx"
  },
  {
    "objectID": "posts/walkability-4/index.html#technical-details",
    "href": "posts/walkability-4/index.html#technical-details",
    "title": "Walking in Wellington - insights from modelling",
    "section": "",
    "text": "Technical challenge\nCovered in\n\n\n\n\nGeoprocessing to get accessibility within a suburb\nJupyter Notebook\n\n\nExtracting accessibility distributions by suburb\nBlog Post\n\n\nBuild a Bayesian model for an individual suburb\nBlog Post\n\n\nModel average (\\(\\mu\\)) and heterogeneity (\\(\\sigma\\)) on two levels: (1) per suburb and, (2) across all suburbs\nBlog Post\n\n\nStan models for Bayesian analysis\nCode files\n\n\nUse average and heterogeneity, relative to Wellington average, to classify accessibility characteristic for a given suburb\nBlog Post\n\n\n\n\n\n\n\n\nDataset\nFormat\nLink\n\n\n\n\nWCC playground locations\n.zip\nWellington City Council\n\n\nWCC suburb boundaries\n.gdb\nWellington City Council\n\n\nStatsNZ Area Unit boundaries\n.gdb\nStatsNZ\n\n\nStatsNZ 2019 meshblock boundaries\n.gdb\nStats NZ\n\n\nLINZ residential polygons\n.gdb\nLINZ\n\n\nWellington street network without elevation\n-\nOpenStreetMap via osmnx\n\n\nWellington street network with elevation\n-\nOpenStreetMap + Google Elevation API via osmnx"
  },
  {
    "objectID": "posts/walkability-4/index.html#accessibility-by-wellington-suburb",
    "href": "posts/walkability-4/index.html#accessibility-by-wellington-suburb",
    "title": "Walking in Wellington - insights from modelling",
    "section": "Accessibility by Wellington suburb",
    "text": "Accessibility by Wellington suburb\nIn all the previous posts (and series), we’ve only looked at accessibility for all of Wellington. Here, we show that the accessibility data can be filtered by the spatial boundary of choice. This is done by some clever geoprocessing enabled by geopandas.\nThe steps are pretty simple: - Extract node coordinates and accessibility from pandana network - Convert node coordinates to a geoseries - Tag nodes within a suburb boundary using the contain operation.\n# Extract node coordinates and accessibility from pandana network\norig_nodes = network_hills.nodes_df\ndf_joined = pd.merge(orig_nodes.reset_index(),\n                     total_hills_1.reset_index(),\n                     how='inner')\ndf_joined.columns = ['node_id', 'lon', 'lat', 'accessibility']\n\n# Convert lat and lon to geoseries\ndf_joined_coords = dp.coords_df_to_geopandas_points(df_joined,\n                                                    crs={'init': 'epsg:4167'})\n\n# Get suburb values for accessibility data\nplayground_df = geopandas.sjoin(wcc_suburbs,\n                                df_joined_coords,\n                                op='contains').drop('index_right', axis=1)\n\nVisualising accessibility within suburb boundaries\nWith a dataframe containing both the accessibility information and the suburb, we can filter and plot the accessibility for specific suburbs. We only need the accessibility data for the suburb and the boundary information to overlay the two datasets.\nkarori_accessibility = playground_df[playground_df['suburb'] == 'Karori']\nkarori_boundary = wcc_suburbs[wcc_suburbs['suburb'] == 'Karori']\n\n\n\nExtracting accessibility distributions by suburb\nThe same filters used to plot the accessibility heatmap within a suburb can be used to extract the accessibility values alone without any spatial information. This raw accessibility data is the basis for our statistical model."
  },
  {
    "objectID": "posts/walkability-4/index.html#modelling-of-accessibility-in-a-single-suburb",
    "href": "posts/walkability-4/index.html#modelling-of-accessibility-in-a-single-suburb",
    "title": "Walking in Wellington - insights from modelling",
    "section": "Modelling of accessibility in a single suburb",
    "text": "Modelling of accessibility in a single suburb\nOnce we have the accessibility values by suburb, we can start thinking of how we might model them. From the previous figure, the suburban accessibility distributions have some distinctive features: - Values are all positive - Some are heaviliy skewed - Most have single mode - Some look like a normal distribution\nuni_norm_model = su.load_or_generate_stan_model('stan', 'univariate_normal')\nlower_trunc_norm_model = su.load_or_generate_stan_model('stan', 'lower_truncated_univariate_normal')\n\nNormal Model\nGiven the unimodel nature of the majority of the accessibility distributions, a normal model is a trivial starting point. We won’t go into the details here but we can easily write a Stan model to compute the \\(\\mu\\) and \\(\\sigma\\) values of a univariate normal distribution. Results from a Stan fit are given below. I’ve also computed the posterior predictive - the ‘y_pred’ variable.\nInference for Stan model: anon_model_cc3fc1beb21cbbe7b94ad66105c98210.\n4 chains, each with iter=2000; warmup=1000; thin=1;\npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\nmu      22.99  6.3e-3   0.35   22.3  22.75  22.99  23.23  23.67   3113    1.0\nsigma   14.28  4.3e-3   0.26  13.79   14.1  14.27  14.45  14.79   3630    1.0\ny_pred  22.71    0.23   14.3  -5.62  12.89  22.82  32.59  50.67   4000    1.0\nlp__    -5086    0.02    1.0  -5089  -5087  -5086  -5085  -5085   1809    1.0\n\nSamples were drawn using NUTS at Mon Mar 18 15:07:35 2019.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at\nconvergence, Rhat=1).\n\n\nTruncated Normal model for better fit\nThe posterior predictive is a useful diagnostic as it reverse-engineers data from the specified model. Checking this generated data against our original data is a very useful tool to assess the suitablity of a model.\nThe normal model approximation for the Karori is seen in the LHS figure below. The RHS shows the raw accessibility values for Karori. It’s quite clear that the Normal approximation fails because it generates negative accessibilities.\n\nA truncated Normal distribution can easily address this issue. Implementing the truncated Normal model variation in Stan is easy - we just need to specify the appropriate bounds for the Normal distribution: lower, upper or both. The only aspect that needed some research was generating posterior predictive samples. But helpfully others have encountered this before and there was a useful discourse in the Stan forums.\nThe figure below shows the posterior predictive samples generated from a Normal and truncated Normal distribution respectively. The truncated Normal can be judged a good fit as it matches the distinctive features of the accessibility data.\n\n\n\nNormal model\nTruncated Normal model"
  },
  {
    "objectID": "posts/walkability-4/index.html#a-collective-bayesian-model",
    "href": "posts/walkability-4/index.html#a-collective-bayesian-model",
    "title": "Walking in Wellington - insights from modelling",
    "section": "A collective Bayesian model",
    "text": "A collective Bayesian model\nModelling every suburb on its own is doable but more useful is a succinct model that does this while also modeling summary statistics across all suburbs. This type of model is commonly known as a hierarchcal model. Hierarchy comes from the intuitive ordering of the models. Here, we have an overall city level model and many suburban level models.\nGoing through the details of a hierarchical model is beyond the scope of this post but there is a great introduction with Python and Pystan by Chris Fonnesbeck. Note that a multi-level model is synonymous with a hierarchical model.\nWe get two types of posterior distributions from the hierarchical model: - \\(\\mu_{s}\\) and \\(\\sigma_{s}\\) for each suburb. - A single \\(\\mu_{c}\\) and \\(\\sigma_{c}\\) for all suburbs (i.e. city level)\nIt is worth noting that only the top 80% of suburbs (by node count) have been included in the model. This was mainly to get the model to run reasonably quickly.\n\nResults for \\(\\mu\\) and \\(\\sigma\\)\nThe posterior distributions output from the hierarchical model can be visualised in a convenient plot known as a Forest Plot. The mean values of the suburban level posterior distribution are plotted as circles. The relative value is used to order and colour code the circles. The 95% credible interval of each suburban \\(\\mu\\) or \\(\\sigma\\) is also plotted as horizontal bars - but these are usually smaller than the circle. The grey band represents the 95% credible interval for the city level average.\n\n\n\n\\(\\mu\\)\n\\(\\sigma\\)\n\n\n\n\n\n\n\n\n\nA couple of general points stand out clearly:\n\nWellington city averages 12 - 16 minutes total travel time to a playground.\nHowever, the variability in accessibility within suburbs is quite high.\n\n\n\nQuadrant visualisation\nTo pick out suburban character (in terms of playground accessibility), we need a visualisation that (1) focuses on the suburbs that fall outside the ‘average band’ and, (2) can consider \\(\\mu\\) and \\(\\sigma\\) at the same time. A simple 2D plot that can satisfy these criteria is the quadrant matrix.\nWe can modify the standard layout slightly to get both the quadrants and the average bands by: (1) normalising the suburban means (for both \\(\\mu\\) and \\(\\sigma\\)) with the city means and, (2) plotting the city level credible intervals as a cental cross. The resulting quadrants now represent combinations of suburban \\(\\mu\\) and \\(\\sigma\\) relative to the city.\n\nThe quadrants represent accessibility character which have a simplistic interpretation.\n\n\n\n\n\n\n\n\n\nHigh \\(\\mu_{norm}\\)\nLow \\(\\mu_{norm}\\)\n\n\n\n\nLow \\(\\sigma_{norm}\\)\nConsistently good accessibility\nConsistent but poor accessibility\n\n\nHigh \\(\\sigma_{norm}\\)\nPoor accessibility for most areas\nGood accessibility for some areas\n\n\n\nWe can get the suburbs that lie in the 4 quadrants listed above with some simple data filters. The list goes from the best suburbs to the worst in terms of consistent accessibility to playgrounds.\n\n\n\nsuburb\nquadrant\ncharacteristic\n\n\n\n\nTe Aro\nLow \\(\\sigma\\) and \\(\\mu\\)\nConsistently good accessibility\n\n\nNewtown\nLow \\(\\sigma\\) and \\(\\mu\\)\nConsistently good accessibility\n\n\nPipitea\nLow \\(\\sigma\\); High \\(\\mu\\)\nConsistent but poor accessibility\n\n\nHataitai\nLow \\(\\sigma\\); High \\(\\mu\\)\nConsistent but poor accessibility\n\n\nNewlands\nHigh \\(\\sigma\\); Low \\(\\mu\\)\nGood accessibility for some areas\n\n\nTawa\nHigh \\(\\sigma\\); Low \\(\\mu\\)\nGood accessibility for some areas\n\n\nBrooklyn\nHigh \\(\\sigma\\); Low \\(\\mu\\)\nGood accessibility for some areas\n\n\nKhandallah\nHigh \\(\\sigma\\) and \\(\\mu\\)\nPoor accessibility for most areas\n\n\nKarori\nHigh \\(\\sigma\\) and \\(\\mu\\)\nPoor accessibility for most areas\n\n\n\n\n\nSuburbs that don’t fit the model\nPart of the reason that Karori performs so poorly is because our model is a poor fit to the accessibility data. In the figure below, we see that the raw values of accessibility appear to have 3 modes - a feature that is reduced to a single average mode in our model. This reduction causes both the \\(\\mu\\) and \\(\\sigma\\) values to inflate.\n\nOnce again, heterogeneity in suburban characteristic is the reason for the model being a poor fit for Rongotai. Rongotai is a suburb with both residential and industrial areas: the residential areas bordering Kilbirnie and, industrial areas containing Wellington Airport and the large shopping area near Lyall Bay. The dual nature of the suburb is reflected in two distinct modes in the raw accessibility values.\n\n\n\nMaking the model better\nOne way to better model Karori is to reduce the suburb into smaller units: perhaps Area Units (now called Statistical Area 2, SA2). SA2 / area unit boundaries are available from Stats NZ. Unfortunately, these units aren’t engineerd to fit perfectly within suburb since they’re primarily designed as statistical units for the census. We see below that SA2 units for Karori don’t extend as far as the suburban boundaries. However, this can be managed by a convenient overlay of SA2 units within the suburban boundaries. And this new overlay geometry could be used to subset the accessibility data.\nAn even better spatial filter is using the LINZ residential polygons to only model accessibilities that fall within the residential areas of the suburb. This should mitigate some of the high \\(\\sigma\\) issues we’ve seen. In the table below, we see that blue regions are the residential polygons overlaid on the accessibility data for Karori and Rongotai. The regions cover the highest density regions of the suburb and make for a much more intuitive filter than area units.\n\n\n\nSuburb\nArea Unit filter\nLINZ residential filter\n\n\n\n\nKarori\n\n\n\n\nRongotai"
  },
  {
    "objectID": "posts/walkability-4/index.html#conclusions",
    "href": "posts/walkability-4/index.html#conclusions",
    "title": "Walking in Wellington - insights from modelling",
    "section": "Conclusions",
    "text": "Conclusions\nWe’ve done the hard yards in extracting some relevant insights about suburban walkability. The insights have a human-understandable frame and can be used to evaluate suburbs from a citizen’s point of view. However, there are some obvious improvements that can be made; the main one being the removal of accessibility for non-residential areas. This is quite an important update so we’ll cover it in detail in the next post."
  },
  {
    "objectID": "posts/modelling/index.html",
    "href": "posts/modelling/index.html",
    "title": "Modelling - a brief reflection",
    "section": "",
    "text": "While working on the geospatial analysis of walkability, I realised that modelling requires its own introduction and it’s worth taking a high-level persepective into why modelling is useful. I hope to make the case that approximating reality with models allows us to dredge up some useful insights.\nApplications of this post can be seen in the final post of walkability, where we will look at: - The accessibility characteristics of a suburb - Suburban characteristics don’t fit our model approximations. And how we can update our model to better reflect reality.\n\n\nThe best reason for trying our hand at statistical modelling comes from the entertaining and brilliant pedagogue: Ben Lambert.\n\nIn life, noise obfuscates signal. What we see often appears as an incoherent mess that lacks any appearance of logic.\n\n\nStatistical inference is the logical framework we can use to trial our beliefs about the noisy world against data. We formalise our beliefs in models of probability.\n\n\n\nA Student’s Guide to Bayesian Statistics, Ben Lambert (p 17)\n\n\n\nIn his book, Lambert goes on to elaborate the gains acheived from employing a Bayesian approach to statistical inference. Our analysis into accessibility by suburb doesn’t explicitly benefit from a Bayesian approach but I’ve chosen to use it anayway since I’m now completely avowed to The Bayesian Way (Bayes-do?).\n\n\n\nThe core component of statistical inference is a statistical model - often shortened to just model. Common models formalise the data generation process by quantifying the relationhip between inputs and outcomes. For example, linear regression models quanitfy the relationship between a set of user-defined inputs and the possible outcomes given those inputs.\nThe model we’re using in this post is much simpler: we’re considering the probability space of the outcomes - with a particular interest in summary statistics like the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\). As we’ll see, we choose a particular mathematical form to represent the probability space of average (\\(\\mu\\)) and heterogeneity (\\(\\sigma\\)) of accessibility within a suburb.\n\n\n\nIt’s worth noting that not all data-driven questions benefit from statistical modelling. Models can be complicated and difficult to explain to others - even technically-oriented peers. In view of this, some data evangelists advocate a simpler analysis process. Kat Greenbrook highlights how the modelling aspect can be left out for many business analytics questions.\n\n\n\n\n\n\n\n\nData Science cycle\nData Analysis cycle\n“Modelling Silo”\n\n\n\n\n\n\n\n\n\nActions from model output / insights\nActions directly from exploratory analysis\nPointless modelling. No pertinent question and no actions\n\n\n\n\n\nImages © Kat Greenbrook\n\n\n\nData anlysis alone is powerful; exploratory analyses unearth useful insights that can be followed through with business actions. As people who harness data for a purpose, we must constantly evaluate whether the extra complexity of the model layer is adding significant utility and insight.\n\nModelling should never be undertaken if there is not a clear use case for the output.\n\n\n\nKat Greenbrook\n\n\n\nAs someone who has frequently lunged into modelling without a cause, I can attest to the pervasive culture of the ‘Modelling Silo’ in Data Science. This ‘cycle’ is wholly disconnected to pertinent questions and, any useful actionable output.\n\n\n\nNow that we have been cautioned to think before we model, we can identify how models can help better understand playground accessibility in Wellington.\nIn the previous post, we ended with heatmaps of accessibility - defined as total travel time. The heatmaps conveyed a holistic picture of areas with worse accessibility due to the hilly topography. However, we couldn’t pick out any details from the overview. For example, we might care about how our specific neighbourhood compares to another, or our neighbourhood vs. the average for the city.\nComparisons can be done with single point values alone. But, robust comparisons rely on statistical inference - the most classic being the t-test for comparing two means. In the walkability modelling post, we will see how we can robustly compare suburbs using a Bayesian statistical model.\n\n\n\nAdding a model for comparing suburbs has further utility - it can be used for explicit or qualitative decision making. Explicit decisions are appropriate in a business context since executives want to tl;dr the best option. Since this series is more focused on exploration, we’ll be building a qualitative picture guided by metrics and analyses.\n\nCan we summarise the playground accessibility characteristic for a given suburb?\n\nThis question can help understand how “family-friendly” a particular suburb is. Young families could compare the suburb accessibility characteristics to help make the decision for a move or, evaluate whether the suburb is right for their lifestyle.\nFrom this question and potential use, we can desgin the model and outputs for an intuitive comparative analysis. The final model allows for two levels of qualitiative comparison: (1) comparing a single suburb to the city average or, (2) comparing any two suburbs together.\n\n\n\nSince models approximate reality, the difference between the model and reality can also add valuable insight.\n\nWhich suburbs don’t follow the approximation set by the model? Can we use our domain knowledge to understand why?\n\nIn this scenario, mismatch between the data (‘reality’) and the model can help us understand the nature of suburbs better; and use this understanding to update our model for a better representation of reality."
  },
  {
    "objectID": "posts/modelling/index.html#introduction",
    "href": "posts/modelling/index.html#introduction",
    "title": "Modelling - a brief reflection",
    "section": "",
    "text": "While working on the geospatial analysis of walkability, I realised that modelling requires its own introduction and it’s worth taking a high-level persepective into why modelling is useful. I hope to make the case that approximating reality with models allows us to dredge up some useful insights.\nApplications of this post can be seen in the final post of walkability, where we will look at: - The accessibility characteristics of a suburb - Suburban characteristics don’t fit our model approximations. And how we can update our model to better reflect reality.\n\n\nThe best reason for trying our hand at statistical modelling comes from the entertaining and brilliant pedagogue: Ben Lambert.\n\nIn life, noise obfuscates signal. What we see often appears as an incoherent mess that lacks any appearance of logic.\n\n\nStatistical inference is the logical framework we can use to trial our beliefs about the noisy world against data. We formalise our beliefs in models of probability.\n\n\n\nA Student’s Guide to Bayesian Statistics, Ben Lambert (p 17)\n\n\n\nIn his book, Lambert goes on to elaborate the gains acheived from employing a Bayesian approach to statistical inference. Our analysis into accessibility by suburb doesn’t explicitly benefit from a Bayesian approach but I’ve chosen to use it anayway since I’m now completely avowed to The Bayesian Way (Bayes-do?).\n\n\n\nThe core component of statistical inference is a statistical model - often shortened to just model. Common models formalise the data generation process by quantifying the relationhip between inputs and outcomes. For example, linear regression models quanitfy the relationship between a set of user-defined inputs and the possible outcomes given those inputs.\nThe model we’re using in this post is much simpler: we’re considering the probability space of the outcomes - with a particular interest in summary statistics like the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\). As we’ll see, we choose a particular mathematical form to represent the probability space of average (\\(\\mu\\)) and heterogeneity (\\(\\sigma\\)) of accessibility within a suburb.\n\n\n\nIt’s worth noting that not all data-driven questions benefit from statistical modelling. Models can be complicated and difficult to explain to others - even technically-oriented peers. In view of this, some data evangelists advocate a simpler analysis process. Kat Greenbrook highlights how the modelling aspect can be left out for many business analytics questions.\n\n\n\n\n\n\n\n\nData Science cycle\nData Analysis cycle\n“Modelling Silo”\n\n\n\n\n\n\n\n\n\nActions from model output / insights\nActions directly from exploratory analysis\nPointless modelling. No pertinent question and no actions\n\n\n\n\n\nImages © Kat Greenbrook\n\n\n\nData anlysis alone is powerful; exploratory analyses unearth useful insights that can be followed through with business actions. As people who harness data for a purpose, we must constantly evaluate whether the extra complexity of the model layer is adding significant utility and insight.\n\nModelling should never be undertaken if there is not a clear use case for the output.\n\n\n\nKat Greenbrook\n\n\n\nAs someone who has frequently lunged into modelling without a cause, I can attest to the pervasive culture of the ‘Modelling Silo’ in Data Science. This ‘cycle’ is wholly disconnected to pertinent questions and, any useful actionable output.\n\n\n\nNow that we have been cautioned to think before we model, we can identify how models can help better understand playground accessibility in Wellington.\nIn the previous post, we ended with heatmaps of accessibility - defined as total travel time. The heatmaps conveyed a holistic picture of areas with worse accessibility due to the hilly topography. However, we couldn’t pick out any details from the overview. For example, we might care about how our specific neighbourhood compares to another, or our neighbourhood vs. the average for the city.\nComparisons can be done with single point values alone. But, robust comparisons rely on statistical inference - the most classic being the t-test for comparing two means. In the walkability modelling post, we will see how we can robustly compare suburbs using a Bayesian statistical model.\n\n\n\nAdding a model for comparing suburbs has further utility - it can be used for explicit or qualitative decision making. Explicit decisions are appropriate in a business context since executives want to tl;dr the best option. Since this series is more focused on exploration, we’ll be building a qualitative picture guided by metrics and analyses.\n\nCan we summarise the playground accessibility characteristic for a given suburb?\n\nThis question can help understand how “family-friendly” a particular suburb is. Young families could compare the suburb accessibility characteristics to help make the decision for a move or, evaluate whether the suburb is right for their lifestyle.\nFrom this question and potential use, we can desgin the model and outputs for an intuitive comparative analysis. The final model allows for two levels of qualitiative comparison: (1) comparing a single suburb to the city average or, (2) comparing any two suburbs together.\n\n\n\nSince models approximate reality, the difference between the model and reality can also add valuable insight.\n\nWhich suburbs don’t follow the approximation set by the model? Can we use our domain knowledge to understand why?\n\nIn this scenario, mismatch between the data (‘reality’) and the model can help us understand the nature of suburbs better; and use this understanding to update our model for a better representation of reality."
  },
  {
    "objectID": "posts/fuel-station-1/index.html",
    "href": "posts/fuel-station-1/index.html",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "",
    "text": "Journeys begin with a question. A chance discovery of an excellent introduction and tutorial on urban modelling by Sir Alan Wilson led me to think about a more general proposition:\n&gt; How can we measure value derived from spatial placement of amenities?\nThe question of value from amenities is an important one for both the private and public sectors. The public sector might care about value derived from amending a bus route or, adding a new bridge / flyover. The private sector might care about value derived from a new supermarket - with questions around potential cannibalism of sales from nearby branches of the same brand to managing competition in the vicinity. The possibilities are extensive! But to keep this exploration tractable, I’ll just be comparing two brands of fuel stations that are well represented in Wellington, New Zealand: Z and BP.\nThis series is an exploration of simple methods to identify and compare value derived from spatial placement. I’ll be using some well-known spatial techniques (e.g. accessibility analysis) and some lesser used ones (network analysis to discern structure)."
  },
  {
    "objectID": "posts/fuel-station-1/index.html#towards-a-business-question",
    "href": "posts/fuel-station-1/index.html#towards-a-business-question",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "Towards a business question",
    "text": "Towards a business question\nWe can approximate value as usefulness. For fuel station networks, usefulness can be broken down into some high level aspects:\n\nFuel station locations – Coverage\nInteractions of fuel stations with the environment – With Humans: accessibility, convenience – With Other entities (e.g. amenities like cafes, cinema theatres etc.): locale\nFuel station characteristics – Available amenities (e.g. toilets, fuel types, food etc.) - supply – Uptake of availabile amenities (i.e. higher scores for amenities that are used more) - where supply meets\n\nTo make concrete comparisons between competing fuel station brands, we need to break down the high level aspects into proxy metrics, or quantitative analyses.\n\nCoverage – Structure of fuel station inter-connectivity – Average spatial separation between two fuel stations – Nearest neighbours: same brand or competitor?\nInteractions with the ‘human’ environment – Driving accessibility\n\nAll these analyses drive a natural business question that can be tracked throughout the series: &gt; Does Z have better coverage than their competitor(s) in Wellington? If so/not, how?\nThe two part business question is a useful format. It forces the Data Scientist / Analyst to examine all the results and develop a sensible high level answer, and providing the requisite details that the answer derives from."
  },
  {
    "objectID": "posts/fuel-station-1/index.html#the-fuel-stations-series-at-a-glance",
    "href": "posts/fuel-station-1/index.html#the-fuel-stations-series-at-a-glance",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "The Fuel Stations series at a glance",
    "text": "The Fuel Stations series at a glance\nThe above analyses are split into 3 posts since the entirity is almost unreadable:\n\nPart 1 (this post) – Introduction to spatial data – Getting fuel station data from OpenStreetMap – Plotting fuel stations on a map\nPart 2 – Abstracting spatial networks into networks – Calculating inter-connectivity metrics\nPart 3 – Driving accessibility analysis"
  },
  {
    "objectID": "posts/fuel-station-1/index.html#resources",
    "href": "posts/fuel-station-1/index.html#resources",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "Resources",
    "text": "Resources\nI’ve used studies from urban data science as inspiration. Some resources I’ve consulted include: - Talk on Urban Data Science by Dr. Cecilia Mascolo - Understanding Traffic with Open Data by researchers at Oxford Internet Institute - Geoff Boening’s blog - especially his package OSMnx - Proximity and accessibility analyses with Pandana\nAll the code and formalised report of the analysis can be found in a github repo. The text is almost identical to the blog posts."
  },
  {
    "objectID": "posts/fuel-station-1/index.html#introduction-to-spatial-data",
    "href": "posts/fuel-station-1/index.html#introduction-to-spatial-data",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "Introduction to Spatial data",
    "text": "Introduction to Spatial data\nSpatial data includes geographical information for physical entities in our world. Since we’re focusing on man-made entities like fuel stations, the simplest geographical information we require is geolocation. This information can be enriched with attributes that describe local geography - derived from the surrounding environment.\n\nWhat spatial data do we need?\nWe need the following data to calculate the coverage metrics and do the quantitative analyses. There are two key types of data: Base and Points of Interest (POIS). The tools used to get and process the data are described in more detail in the following section.\n\n\n\n\n\n\n\n\n\nData\nType\nWhy?\nHow?\n\n\n\n\nGeo-tagged fuel stations\nPOIS\nThe key spatial entities we’re interested in\nOpenStreetMap via Overpass\n\n\nRegional map and street network\nBase\nTo connect the fuel stations via real world streets and roads\nOpenStreetMap via OSMnx\n\n\nStreet network broken up into regular points\nBase\nFor simpler interactions between geography and fuel stations\nOpenStreetMap via Panadana\n\n\n\nOpenStreetMap (OSM) is the underlying data source. OSM is an open, collaborative map of the world. Map data can be queried and downloaded locally using the Overpass API. In a later section, I will show how I leveraged the collaborative feature to update / add information on Wellington fuel stations. Editing OSM will continue to be an important aspect of any future version of this project, e.g. the available services at the various fuel stations.\nThe main advantage of using OSM, other than the altruistic aspect of enriching the available information for others, is that the same framework provides data for all fuel station brands. Comparative analyses become much easier since they don’t require data munging from different sources.\n\n\nSummary of spatial analyses\nAs indicated in the previous section, there are two main data types: Base and POIS. At a simplistic level, the analyses outlined in this report are essentially different types of interactions between the base data and POIS. I’ve focused on pairwise interactions involving: - POIS - POIS interactions via the base layer – Distances between fuel stations - Base grid - POIS interactions – Distances between surrounding street geography (reduced to points) and fuel stations\nWhile these interactions seem trivial, they can generate considerable insight into coverage.\n\nPOIS - POIS interactions via the base layer\n\nConnectivity structure of the fuel station network\n\nBase grid - POIS interactions\n\nAccessibility to fuel stations"
  },
  {
    "objectID": "posts/fuel-station-1/index.html#tools",
    "href": "posts/fuel-station-1/index.html#tools",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "Tools",
    "text": "Tools\nThis series is generated with Python 2.7 running in a Jupyter Notebook. All the Python packages highlighted in this section can be easily installed using a package manager like conda or pip.\n\n\n\n\n\n\n\n\nTool\nType\nWhat does it do?\n\n\n\n\nJupyter Notebook\nGeneral\nAllows for the creation of an annotated, executable analysis. Like this one!\n\n\nPandas\nGeneral\nEnables data to be stored, manipulated and analysed in dataframes (like R).\n\n\nMatplotlib & Seaborn\nGeneral\nGeneral and ggplot2-like plotting libraries\n\n\nNetworkx\nGeneral\nFor standard network analysis when we don’t require spatial information.\n\n\nOSMnx\nSpatial\nFor analysis of streets and roads with network algorithms.\n\n\nPandana\nSpatial\nFor fast and efficient accessibility analyses. Need 0.4.1 / 0.4.0"
  },
  {
    "objectID": "posts/fuel-station-1/index.html#get-spatial-data",
    "href": "posts/fuel-station-1/index.html#get-spatial-data",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "Get Spatial data",
    "text": "Get Spatial data\nThere 3 basic steps to getting spatial data from OSM: - Defining a region of interest via a bounding box (we rarely want to extract data for the whole world!) - Creating the data query with filters for particular spatial entities (here, we care only about fuel stations) - Converting the query results into a manageable data structure for analysis\n\nSet bounding box\nA bounding box of lattitude and longitude coordinates describes a rectangular geospatial region. For this report, I’ve chosen a bounding box that includes Wellington City and some of Lower Hutt. This selection is important since only the entities within the bounding box are used in the analysis. The visual tool here is useful for obtaining the bounding box coordinates from a user-defined rectangle on the map.\nA key technical point is that bounding box conventions do vary: - The general definition uses (min Longitude , min Latitude , max Longitude , max Latitude), or (W, S, E, N) - Pandana and Overpass use (S, W, N, E).\n\n\n\npng\n\n\n\n\nCreate Query\nThe following section creates a query to get fuel station data from OSM. The tags list can also be amended to get other amenities. The full list is here. For example, we can easily get data for cafes and restaurants by adding these to the tags list.\nThe Overpass API query is not very easy to read but the main components are: - The bounding box: the area where we want the search performed. - Data Primitives: ways, nodes, tags, relations.\nThe data primities of OSM have an intrinsic hierarchy with nodes being the root primitive. - Nodes: Single point with explicit [lat, lon] coordinates. Root primitive - Ways: Collection of nodes that defines a polygon (e.g. a building) or polyline (e.g. a road). - Relations: Represent the relationship of existing nodes and ways - Tags: Metadata stored as key-value pairs.\nThe main primitives used in this report are ways, nodes and tags. The tags are used to filter specifically for fuel stations. More information about the entities of Open Street Maps can be found here.\n\n\nGetting data from Overpass\nOnce the query is constructed and sent with a ‘GET’ request, the resulting JSON is reshaped as a Pandas dataframe that contains relevant metadata about each fuel station.\n\n\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlon\nname\noperator\nbrand\ntype\n\n\n\n\n2845230323\n-41.325288\n174.810883\nNaN\nNaN\nNaN\nnode\n\n\n2845230324\n-41.325284\n174.811057\nNaN\nNaN\nNaN\nnode\n\n\n2845230322\n-41.325275\n174.810774\nNaN\nNaN\nNaN\nnode\n\n\n2845230321\n-41.3252\n174.810729\nNaN\nNaN\nNaN\nnode\n\n\n5821475056\n-41.325128\n174.81092\nZ Broadway\nNaN\nZ\nnode\n\n\n\n\n\nGet specific fuel stations\nThe data downloaded from OSM (via Overpass) includes all nodes and ways tagged as ‘fuel’. The brand of the fuel station can be be used to filter for station-specific analysis. In NZ, there are 4 retailer brands: Z, Caltex, BP and Mobil. Since the merger in 2016, Z and Caltex can be regarded as two brands from a single entity. Additional details of brand coverage here. In this preliminary version of the analysis, I’ve only include the explicit Z branded fuel stations. For a more general analysis of the Z entity, we’d also need to include the Caltex branded fuel stations.\n\n\nData Issues\nTo check the data, we can query the Wellington fuel stations dataset by brand / operator to only get those that are associated with Z. The query returns 13 Z stations within the search region. From a cursory glance at the named Z stations and the list from the website, we can see that there is considerable parity. Despite the close parity however, there are some issues with the data: - Inconsistency between the operator and brand attributes. - No geolocation for some stations.\n\n\n\npng\n\n\nThe key problem with the data is that a significant portion of the stations don’t have location coordinates. This problem stems from the two main types of OSM topological entities: ways and nodes. Depending on how a user marks out the location of a fuel station, the entity can be either a way or a node.\n\nIf the station is marked with a single point, the entity is a node with a clear geolocation.\nIf the station’s perimeter / main building is traced out as a polygon, the entity is a way with no clear geolocation.\n\n\n\nResolving data issues\nSince the underlying problem is a data issue, we can add / edit the data ourselves. You can sign up and verify your email as an OSM editor - quite easy to do. Once I got the permission to edit OSM, I simply went in and added / updated the nodes for the Z fuel stations. Since I need the BP list to be accuate as well, I also edited (where required) the data for BP stations in the Wellington bounding box. I should mentiona that we can actually convert ways to a centroid (a single lat, lon value at the centre of the polygon) with Geopandas. However, I didn’t know about this helpful feature when I was working through the fuel station analysis! The polygon centroid method will be introduced in a later post on geo-munging."
  },
  {
    "objectID": "posts/fuel-station-1/index.html#fuel-stations-in-wellington",
    "href": "posts/fuel-station-1/index.html#fuel-stations-in-wellington",
    "title": "Geospatial proximity analysis with fuel stations - data",
    "section": "Fuel stations in Wellington",
    "text": "Fuel stations in Wellington\nAfter editing the data in OSM, the corrected list of Z stations is now at parity with the Z website. With a similar brand / operator filter, we can create an equivalent dataframe for BP stations.\n\n\n\n\n\n\n\n\n\n\n\n\nid\nlat\nlon\nname\noperator\nbrand\ntype\n\n\n\n\n5821475056\n-41.325128\n174.81092\nZ Broadway\nNaN\nZ\nnode\n\n\n3120151445\n-41.320054\n174.794407\nZ Kilbirnie\nNaN\nZ\nnode\n\n\n5821475059\n-41.314924\n174.813972\nZ Miramar\nNaN\nZ\nnode\n\n\n5821475061\n-41.313163\n174.781812\nZ Constable Street\nNaN\nZ\nnode\n\n\n5821475058\n-41.297146\n174.776556\nZ Taranaki Street\nNaN\nZ\nnode\n\n\n5544110098\n-41.294501\n174.774397\nZ Vivian St\nNaN\nZ\nnode\n\n\n5821475063\n-41.281636\n174.778417\nZ Harbour City\nNaN\nZ\nnode\n\n\n5821475060\n-41.25602\n174.765535\nZ Crofton Downs\nNaN\nZ\nnode\n\n\n2206248455\n-41.236226\n174.906171\nZ Seaview\nNaN\nZ\nnode\n\n\n331132009\n-41.2263\n174.806795\nZ Johnsonville\nNaN\nZ\nnode\n\n\n5821475057\n-41.222778\n174.868833\nZ Petone\nNaN\nZ\nnode\n\n\n2118620317\n-41.214312\n174.887163\nZ Hutt Road\nNaN\nZ\nnode\n\n\n5821475062\n-41.204023\n174.914085\nZ VIC Corner\nNaN\nZ\nnode\n\n\n319121061\n-41.197885\n174.937446\nZ High Street\nZ\nNaN\nnode\n\n\n\n\nVisualising fuel stations\nNow that we have some geolocation data, it’s a useful exercise the plot them on a map. Folium is a great package for embedding interactive Leaflet apps into Jupyter notebook and is also web compatible with no additional steps. All this functionality with just 4 lines of Python code! The maps below show that Z and BP stations are well dispersed around the Wellington metropolitan area - which looks rather like a fish hook (hei matau)! There are clear differences in coverage but it’s hard to articulate these differences with only a simple point map. In the next two posts of the series, we’ll see how to extract quantitative, comparable insights about coverage.\n\nZ Stations\n\n\n\n\n\n\nBP Stations"
  },
  {
    "objectID": "posts/walkability-1/index.html",
    "href": "posts/walkability-1/index.html",
    "title": "Walking in Wellington",
    "section": "",
    "text": "This post introduces a series of walkability analyses for Wellington - with a specific focus on walking to playgrounds. The analyses will allow us to examine the following questions:\nBut to understand the importance of these specific questions, we need to zoom out to a more high level perspective."
  },
  {
    "objectID": "posts/walkability-1/index.html#benefits-of-an-urban-life",
    "href": "posts/walkability-1/index.html#benefits-of-an-urban-life",
    "title": "Walking in Wellington",
    "section": "Benefits of an urban life",
    "text": "Benefits of an urban life\nIn his recent book, Order without Design, Alain Bertaud beautifully lays out the value proposition of urban life:\n\n\n\nA commute short enough that one has time for leisure activities.\n\n\nAn open job market that allows one to change jobs and - through trial and error - find a job that best suits.\n\n\nA residence from which access to social life or nature is quick and easy.\n\n\n\n\n\nOrder without Design, Alain Bertaud (p 19)\n\n\n\nAs a life-long urbanite with stints around the world, I have given thought to each of these points when evaluating a place to live - from the city itself to a suburb / area. I even moved back to New Zealand after a few years in the UK solely for #2 and #3. As I was looking for rental property, #1 and #3 were primary on my mind.\nThe above three points are typical core values for individuals. For individuals in a family, #3 would grow to include other amenities like schools, medical services, retail etc. A comprehensive evaluation of amenities can be found in metrics like the NDAI (Neighbourhood Destination Accessibility Index)."
  },
  {
    "objectID": "posts/walkability-1/index.html#accessing-amenities-by-passive-or-active-transport",
    "href": "posts/walkability-1/index.html#accessing-amenities-by-passive-or-active-transport",
    "title": "Walking in Wellington",
    "section": "Accessing amenities by passive or active transport",
    "text": "Accessing amenities by passive or active transport\nAs urbanites, we can access an amenity with several modes of passive or active transport. Passive transport is usually motorised transport like cars, buses, trains or hired vehicles (taxi or Uber). By contrast, active transport is a physical activity - including walking, cycling, running, and skateboarding. Enabling and encouraging active transport modes has become particularly relevant in the current climes.\n\n\nReducing car reliance and encouraging more transport-related physical activity are now recognised as beneficial objectives from health, social and environmental perspectives. Evidence is accumulating that a number of built environment attributes are associated with the likelihood of residents using active transport.\n\n\n\n– Measuring neighbourhood walkability in NZ cities\n\n\nThe above quote from a research paper published by Knowledge Auckland succinctly summarises the connection between transport, physical activity and the built urban environment. Urban attributes are critical for understanding if people will substitute automobile transport (particularly the car) for any choice of active transport.\n\n\nValid and reliable measures of these urban attributes are critical for improving our understanding of the relationship between the built environment and transport mode use.\n\n\n\n– Measuring neighbourhood walkability in NZ cities"
  },
  {
    "objectID": "posts/walkability-1/index.html#measuring-walkability-to-amenities",
    "href": "posts/walkability-1/index.html#measuring-walkability-to-amenities",
    "title": "Walking in Wellington",
    "section": "Measuring walkability to amenities",
    "text": "Measuring walkability to amenities\nIn this series, the analyses will be constrained to walking - rather than a comprehensive view of active transport. The combined impact of urban attributes that enable walking can be measured objectively with Walkability metrics. WalkScore is an implementation of walkability. The Walkability Index, described in Measuring Neighbourhood Walkability in New Zealand Cities, is another.\nWalkability typically presents a comprehensive picture. But at its core is the predominant question that encapsulates opportunity cost for any transport mode:\n\nHow long will it take me?\n\nWhile we all consider itinerary-specific travel times, a general view of travel times to amenities can be quantified and visualised with accessibility heatmaps."
  },
  {
    "objectID": "posts/walkability-1/index.html#walkability-by-example-walking-to-playgrounds-in-wellington",
    "href": "posts/walkability-1/index.html#walkability-by-example-walking-to-playgrounds-in-wellington",
    "title": "Walking in Wellington",
    "section": "Walkability by example: walking to playgrounds in Wellington",
    "text": "Walkability by example: walking to playgrounds in Wellington\nThis introductory post is followed by a series that will examine walkability to playgrounds in Wellington using accessibility analyses by travel time. Playgrounds are an important recreation resource and are typically accessed on foot. The posts will consider just how reasonable their pedestrian accessibility actually is.\nThe topic is narrow, but intended for covering concepts in depth. The following posts will cover:\n\nThe impact of topography (hills) on travel times.\n\nHow prohibitive is Wellington’s topography on pedestrian accessibility?\n\nModelling and visualising differences between suburbs.\n\nCan we interpret pedestrian accessibility to playgrounds with a ‘family-friendly’ angle?\n\nAccessibility to council playgrounds vs. school playgrounds\n\nDoes the inclusion of school playgrounds improve accessibility? By how much?\n\nThe impact of adding new playgrounds on travel times.\n\nWhat is the impact of the new Berhampore playground on accessibility in Berhampore and nearby suburbs?\n\n\nThe final post will close the series by examining how accessibility analyses can inform about the “liveability” of an urban environment."
  },
  {
    "objectID": "posts/maritime-data-enrichment/index.html",
    "href": "posts/maritime-data-enrichment/index.html",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "",
    "text": "The value and challenge of data engineering in the public sector is creating useful real-world analogues as enriched columns or entity tables when commercial data is unavailable. With judicious data engineering, independent data sources can provide myriad perspectives with real-world relevance in analyses and modelling.\nOne of the datasets, I’ve been able to explore is cleaned ship movements (derived from AIS data) through an organisation subscription. The cleaned data provided was split into:\n\nSpatio-temporal point data of the movements - ship tracks\nSpatio-temporal point data of stops - port visits\n\nWhile the bulk of the difficult data cleaning work was already done by the provider, preparing the data for analysis useful for policymakers required additional data wrangling."
  },
  {
    "objectID": "posts/maritime-data-enrichment/index.html#introduction",
    "href": "posts/maritime-data-enrichment/index.html#introduction",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "",
    "text": "The value and challenge of data engineering in the public sector is creating useful real-world analogues as enriched columns or entity tables when commercial data is unavailable. With judicious data engineering, independent data sources can provide myriad perspectives with real-world relevance in analyses and modelling.\nOne of the datasets, I’ve been able to explore is cleaned ship movements (derived from AIS data) through an organisation subscription. The cleaned data provided was split into:\n\nSpatio-temporal point data of the movements - ship tracks\nSpatio-temporal point data of stops - port visits\n\nWhile the bulk of the difficult data cleaning work was already done by the provider, preparing the data for analysis useful for policymakers required additional data wrangling."
  },
  {
    "objectID": "posts/maritime-data-enrichment/index.html#generalising-concepts-from-commerical-container-shipping",
    "href": "posts/maritime-data-enrichment/index.html#generalising-concepts-from-commerical-container-shipping",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Generalising concepts from commerical container shipping",
    "text": "Generalising concepts from commerical container shipping\nShipping lines manage container ships like buses but with greater tactical and strategic agility as they are highly competitive markets. Ships are associated with a vessel fleet that make journeys for a given service to pre-set schedules. For example, a weekly ANL shipping line service (KIX) connecting New Zealand, Australia and Southeastern Asia.\n\n\nService: an ordered set of port visits with an associated fleet that is marketed by a shipping line. In the example above, the KIX service between New Zealand, Australia and Southeastern Asia marketed by the shipping line, ANL.\nJourney: a single rotation done by a ship in the service.\nSchedule: frequency and duration of service.\n\nThe details of commercial offerings from shipping lines cannot be connected with an independent data source like AIS. Instead, we define a series of data enrichments that are conceptually similar and keep a higher level that can generalise across all types of service offerings.\n\nVoyage: an ordered set of international port visits made by a given ship that starts and ends in New Zealand.\nRoute: an ordered set of connected seaboards (sub-regions) based on the ports visited by the ship on a given voyage.\nSchedule: a derived timetable for a ship built from the voyages and associated routes in a given period of time e.g. 2021."
  },
  {
    "objectID": "posts/maritime-data-enrichment/index.html#breaking-journeys---from-visits-to-voyages",
    "href": "posts/maritime-data-enrichment/index.html#breaking-journeys---from-visits-to-voyages",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Breaking journeys - from visits to voyages",
    "text": "Breaking journeys - from visits to voyages\nThe first step in data processing was to puncutate a mass of port visits made by a ship in some unit of time into a voyage. Fortunately, a simple algorithm can separate port visits into voyages for container ships since they run like buses to a fixed schedule.\n\nLike buses, a particular ship can be pulled into another schedule.\nUnlike bus routes, there can be minor changes to the set of visited ports across the year for the same schedule.\n\nThe algorithm identifies a voyage as the sequence of international ports visited between the export and import ports in New Zealand on a single ship journey. Voyages exclude New Zealand ports visited by the ship for cabotage as the policy focus during the COVID-19 pandemic has been on international rather than domestic connectivity.\n\nThe export port is the last New Zealand port before the ship departs for an international port on its outward journey.\nThe import port is the first New Zealand port on the ship’s inward journey.\n\n\nThe algorithm extracts voyages for ships that make at least two separate journeys to New Zealand. A ship that makes a solitary journey to New Zealand from its typical service schedule elsewhere in the world will be not be included. These types of solitary voyages will need additional business logic to isolate the port visits into the ones relevant for the atypical voyage to New Zealand.\nSplitting a contiguous series of port visits into discrete voyages also offers the opportunity of classifying the voyage into a route based on the seaboards of the ports visited in the voyage."
  },
  {
    "objectID": "posts/maritime-data-enrichment/index.html#connecting-seaboards-with-routes",
    "href": "posts/maritime-data-enrichment/index.html#connecting-seaboards-with-routes",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Connecting seaboards with routes",
    "text": "Connecting seaboards with routes\nWe have definted routes as a higher order entity as they can aggregate across voyages in a way that mimics liner shipping services set up by shipping lines but with the added benefit of being insensitive to specific commercial offerings.\nEvery voyage can be classified as belonging to a route based on the (alphabetically ordered) set of unique visited regions. We use the region classification to approximate a seaboard UN geoscheme at the subregion level based on the country of the visited port.\nThe exceptions of this classification are: - dis-aggregating Australia and New Zealand - aggregating Polynesia, Micronesia and Melanesia to Oceania.\nThe ordered set reduces the high variability in the order and number of visited ports in the same region across similar voyages. . For example, a ship that visits Auckland, Melbourne, Brisbane, Shanghai and Busan belongs to the Australia-Eastern Asia-New Zealand route. A ship that visits Tauranga, Sydney, Hong Kong, Ningbo and Tokyo will also be part of the same route Australia-Eastern Asia-New Zealand."
  },
  {
    "objectID": "posts/maritime-data-enrichment/index.html#adding-enrichment-to-ship-tracks",
    "href": "posts/maritime-data-enrichment/index.html#adding-enrichment-to-ship-tracks",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Adding enrichment to ship tracks",
    "text": "Adding enrichment to ship tracks\nSince the port visits are a reduced version of the ship tracks focused on stop points, they are the first step for data enrichment. However, both data sets are complementary and provide different perspectives of ship movement.\n\nThe enrichment of voyages and routes in port visits can be joined to the spatio-temporal ship tracks data on ship name. Fanout is removed with a time filter - only tracks within the time span of a given voyage are kept in the data."
  },
  {
    "objectID": "posts/infrastructure-flows/index.html",
    "href": "posts/infrastructure-flows/index.html",
    "title": "Infrastructure Flows",
    "section": "",
    "text": "In the previous post, I blogged about the challenge of working with spatial networks. Once overcome, the power of spatial network analytics is easily accessed. In this post, I work through a common request in the transport sector: how do a set of origin destination flows aggregate on the transport infrastructure? A whole set of transport relevant policy questions around critical network points and resilience can follow flow calculations.\nThe importance of loading flows on the network have already been acknowledged by Robin Lovelace and Mark Padgham in their packages stplanr and dodgr. The former was much more relevant to me but the network processing challenge (as highlighted in the previous post) was a key blocker. Now that the network connectivity has been temporarily addressed, I thought it would be an interesting exercise to see if the sfnetworks package itself allows for aggregation of routes into flows."
  },
  {
    "objectID": "posts/infrastructure-flows/index.html#spatial-networks-the-power",
    "href": "posts/infrastructure-flows/index.html#spatial-networks-the-power",
    "title": "Infrastructure Flows",
    "section": "",
    "text": "In the previous post, I blogged about the challenge of working with spatial networks. Once overcome, the power of spatial network analytics is easily accessed. In this post, I work through a common request in the transport sector: how do a set of origin destination flows aggregate on the transport infrastructure? A whole set of transport relevant policy questions around critical network points and resilience can follow flow calculations.\nThe importance of loading flows on the network have already been acknowledged by Robin Lovelace and Mark Padgham in their packages stplanr and dodgr. The former was much more relevant to me but the network processing challenge (as highlighted in the previous post) was a key blocker. Now that the network connectivity has been temporarily addressed, I thought it would be an interesting exercise to see if the sfnetworks package itself allows for aggregation of routes into flows."
  },
  {
    "objectID": "posts/infrastructure-flows/index.html#set-up",
    "href": "posts/infrastructure-flows/index.html#set-up",
    "title": "Infrastructure Flows",
    "section": "Set up",
    "text": "Set up\nAll code for this post can be found on github. Dataset and package dependencies are all included in the repository directly. The requirements are the same as the previous post.\n# kiwirail data from \n# https://data-kiwirail.opendata.arcgis.com/datasets/kiwirail-track-centreline\n\n# Basic filter and transform of rail network from Kiwirail\n# only keeping tracks\nnz_rail &lt;- st_read(here::here(\"data\", \"kiwirail.gdb\")) %&gt;%\n  filter(!Type %in% c(\"Crossover\", \"Yard Track\")) %&gt;%\n  st_transform(2193) %&gt;% \n  st_cast(\"LINESTRING\")\n\n# nz port locations\nport_locs &lt;- read_csv(here::here(\"data\",\n                                 \"port_locations.csv\")) %&gt;%\n  st_as_sf(coords=c(\"lon\", \"lat\"), crs=4326) %&gt;%\n  filter(country == \"New Zealand\") %&gt;%\n  filter(LOCODE %in% c(\"NZAKL\", \"NZTRG\", \"NZNPE\", \"NZWLG\",\n                       \"NZNSN\", \"NZLYT\", \"NZPOE\")) %&gt;%\n  st_transform(crs=2193)\n\n# Convert to sfnetwork\n# rounding coordinates to prevent precision errors\nst_geometry(nz_rail) &lt;- nz_rail %&gt;% \n  st_geometry() %&gt;% \n  map(~round(., -1)) %&gt;% \n  st_sfc(crs = st_crs(nz_rail))\n\n# subdividing edges where intersections happen at \n# internal points. excluding this creates a disconnected network\n# because sfnetwork only connects edges where edges join at the \n# terminal nodes\nrailway_net &lt;- nz_rail %&gt;%\n  as_sfnetwork(directed=F) %&gt;% \n  convert(to_spatial_subdivision) %&gt;% \n  convert(to_spatial_simple)\n\n# Function for route calculation \n# returns a function that uses sfnetwork global variable\nst_network_paths_mod &lt;- function(from, to){\n  return(try(st_network_paths(railway_net, from, to)))\n}"
  },
  {
    "objectID": "posts/infrastructure-flows/index.html#create-origin-destination-od-data",
    "href": "posts/infrastructure-flows/index.html#create-origin-destination-od-data",
    "title": "Infrastructure Flows",
    "section": "Create origin destination (OD) data",
    "text": "Create origin destination (OD) data\nIn this example, we don’t have a pre-set OD matrix so we need to create our own. For some aspects of my work, I’m interested in freight flows between New Zealand ports so I have created a simple example of that use case. The steps to create an OD matrix are a simple extension of the previous post - the main differences being:\n\nthe crossing function to create all combinations of two vectors and,\na simple rnorm function to generate some fake flows per origin-destination combination.\n\n# Isolate nodes as sf dataframe\nnodes_rail &lt;- railway_net %&gt;% activate(\"nodes\") %&gt;% st_as_sf()\n\n# Create origin destination (OD) for all port-port combinations\n# Create dummy 'flow' values for every OD pair\norig_dest &lt;- crossing(from = port_locs %&gt;%\n  st_set_geometry(NULL) %&gt;% pull(LOCODE),\n  to = port_locs %&gt;%\n    st_set_geometry(NULL) %&gt;% pull(LOCODE)) %&gt;%\n  inner_join(port_locs %&gt;% select(LOCODE), by = c(\"from\"=\"LOCODE\")) %&gt;%\n  rename(from_port = geometry) %&gt;%\n  inner_join(port_locs %&gt;% select(LOCODE), by = c(\"to\"=\"LOCODE\")) %&gt;%\n  rename(to_port = geometry) %&gt;%\n  rowwise() %&gt;%\n  mutate(flow = (rnorm(1) * 100) %&gt;% abs())  %&gt;%\n  mutate(to_index = st_nearest_feature(to_port, nodes_rail), \n         from_index = st_nearest_feature(from_port, nodes_rail),\n         route = row_number())\n\n# Find shortest path routes\nroutes_df &lt;- orig_dest %&gt;%\n  mutate(path = pmap(list(from = from_index, to = to_index), .f=st_network_paths_mod)) %&gt;% \n  unnest(cols=c(path))"
  },
  {
    "objectID": "posts/infrastructure-flows/index.html#visualise-all-routes",
    "href": "posts/infrastructure-flows/index.html#visualise-all-routes",
    "title": "Infrastructure Flows",
    "section": "Visualise all routes",
    "text": "Visualise all routes\nAll the edges involved in a route are contained in the edge_paths list. Once expanded out fully with unnest, we can join the edge geometries from the sfnetworks dataframe for a simple visualisation. From a cursory glance, it seems like all the routes that could be found have been. Many port-port combinations have no routes because they are on different islands. Terrestrial infrastructure networks in New Zealand have two components - one component for Te Ika a Maui (North Island) and another for Te Waipounamu (South Island). Noting, the existence of the two components because there is no land bridge connecting the islands.\n# Unnest to get one row per edge\n# Join on edge sf dataframe for geometry\nport_port &lt;- routes_df %&gt;% \n  unnest(cols=c(edge_paths)) %&gt;% \n  select(edge_paths) %&gt;% \n  inner_join(railway_net %&gt;% \n               activate(\"edges\") %&gt;% \n               st_as_sf() %&gt;% \n               mutate(edge = row_number()), by = c(\"edge_paths\" = \"edge\"))  %&gt;% \n  st_as_sf()\n\n# Visualise all routes\nggplot() + \n  geom_sf(data = railway_net %&gt;% activate(\"edges\") %&gt;% st_as_sf()) + \n  geom_sf(data=port_port, colour=\"red\")"
  },
  {
    "objectID": "posts/infrastructure-flows/index.html#aggregating-flows",
    "href": "posts/infrastructure-flows/index.html#aggregating-flows",
    "title": "Infrastructure Flows",
    "section": "Aggregating flows",
    "text": "Aggregating flows\nSince the routes dataframe can be unnested to give one edge per route per row, we can group by the edge and summarise the relevant property per edge. The property we’re intersted in is the inherited origin-destination flow value that every edge in particular route inherits. Thus, to get the overall flow passing through any edge in the network, we summarise across all routes passing through it by adding up all the flow values.\n# Group by every edge that has a route passing through\n# Aggregate all routes  that pass through any given edge\n# Aggregation by summing flow values\nflows_df &lt;- routes_df %&gt;% \n  unnest(cols=c(edge_paths)) %&gt;%\n  group_by(edge_paths) %&gt;% \n  summarise(flow = sum(flow))"
  },
  {
    "objectID": "posts/infrastructure-flows/index.html#visualise-flows",
    "href": "posts/infrastructure-flows/index.html#visualise-flows",
    "title": "Infrastructure Flows",
    "section": "Visualise flows",
    "text": "Visualise flows\nSince the flow values between the ports were randomly assigned, there isn’t much meaning in the visualisation. However, one useful aspect to note is the association of the “nearest rail port”. All ports in this example, except one, are on a railway line. Port Nelson (at the top of the South Island) is the only isolated red dot without a grey dot underneath. It’s closest rail node is at Picton. In an actual analysis, we would have to check whether the nearest port identified by the algorithm is meaningful for the use case. It could be that rail traffic might stop elsewhere, that is more conveniently located to a short road transfer route rather than the closest node in the rail network.\n# Visualise aggregated flows\nggplot() + \n  geom_sf(data = railway_net %&gt;%\n            activate(\"edges\") %&gt;% \n            st_as_sf()) + \n  geom_sf(data = railway_net %&gt;% \n            activate(\"edges\") %&gt;%\n            st_as_sf() %&gt;% \n            mutate(row = row_number()) %&gt;% \n            inner_join(flows_df, by = c(\"row\" = \"edge_paths\")),\n          aes(size=flow), \n          colour = \"blue\") + \n  geom_sf(data = railway_net %&gt;% \n            activate(\"nodes\") %&gt;%\n            st_as_sf() %&gt;% \n            mutate(row = row_number()) %&gt;% \n            inner_join(orig_dest, by = c(\"row\" = \"to_index\")), \n          aes(colour = \"red\"),\n          size = 4) + \n  geom_sf(data = orig_dest %&gt;% select(from_port) %&gt;% distinct() %&gt;% st_as_sf(), \n          aes(colour = \"grey\"),\n          size = 4) + \n  scale_colour_identity(breaks = c(\"red\", \"grey\"),\n                        labels = c(\"nearest rail port\", \"original port\"),\n                        guide = \"legend\", \n                        name = \"\")"
  },
  {
    "objectID": "projects/covid-19-economic/index.html",
    "href": "projects/covid-19-economic/index.html",
    "title": "COVID-19 impacts on economic indicators",
    "section": "",
    "text": "The COVID-19 pandemic has affected economic activity around the world. However, direct impacts cannot be calculated with a comparison between the last pre-pandemic year and current pandemic years. For indicators that respond quickly to singular, large shocks like a global pandemic, one approach for extracting a causal impact is to subtract the actual values from a projected counterfactual generated from trends prior to the shock.\n\nEasily available economic indicators of container trade, in terms of TEU (twenty foot equivalent units), are used to estimate the causal impact of the global pandemic that started in 2020 since “..the dynamics of the breakbulk market are related to manufacturing and consumption.” (Geography of Transport Systems Ch. 5.4.2) Imports are connected to consumption (demand) while exports link to manufacturing (supply as a response to external demand).\nFor container imports, the counterfactual predictions from an ETS (exponential smoothing) model show that most of 2020 volumes were considerably below expected. From 2021, the predictions and the actuals fall in a little closer together possibly indicating some recovery.\n\nOverall exports show no clear trend since New Zealand exports primary products - each with different seasonalities and trends. Instead, exports need to be modelled separately by commodity. Wood products show the greatest reductions and ongoing dampening while food commodities have seen little change to expectations.\n\n\nDisclaimer\nThe contents and figures in this post are not official outputs from the Ministry of Transport. They are research-oriented exploratory analyses intended as demonstrations of approaches and techniques relevant to public sector data science."
  },
  {
    "objectID": "projects/travel-activity-patterns/index.html",
    "href": "projects/travel-activity-patterns/index.html",
    "title": "Travel Activity Patterns",
    "section": "",
    "text": "Transport policy that aims to modify travel or activity behaviour must first understand what people do, how, why and with whom. Without it policymakers risk introducing misaligned policies to the desired behaviour change or creating asymmetric distributional impact where some groups are considerably more disadvantaged than others.\nHowever, understanding the intracies of daily travel and activity behaviour is a high dimensional problem requiring reduction in complexity before it becomes useful for testing policy impacts.\nClustering is a common approach that both reduces complexity in the data as well as creating additional meaning from the clusters themselves. With the addition of sequence analysis we can preserve temporal information of activity patterns while allowing individual variability to be aggregated as clusters / typologies. The individuals within the clusters can be joined back to the person dataset for distributional impact analyses.\nAlmost 6,700 days of daily activities done by ~3000 individuals in one year of data (2017) collected by the Household Travel Survey can be neatly summarised into three daily typologies.\n\nWork commuting\nEducation\nMainly, home, errands and leisure\n\n\nGenerating splits further down the dendrogram can unearth more nuance. For example, Work commuting cluster splits into two additional typologies:\n\nEarlybirds at work with diffused morning and evening commuting times.\nRegular 9-5ers with a a strong single morning and evening commute peak at 8 am and 5:30 pm respectively\n\n\n\nDisclaimer\nThe contents and figures in this post are not official outputs from the Ministry of Transport. They are research-oriented exploratory analyses intended as demonstrations of approaches and techniques relevant to public sector data science."
  },
  {
    "objectID": "projects/maritime-connectivity/index.html",
    "href": "projects/maritime-connectivity/index.html",
    "title": "Maritime Connectivity",
    "section": "",
    "text": "Knowing where ships go in order to connect New Zealand to global trading partners provides a qualitative picture of maritime connectivity. We can understand that we are well connected to Australia, Southeastern Asia and Eastern Asia. But how much? Are we better connected via maritime links to some partners than others?\nNetwork analysis offers one way to answer these questions. Networks are a common technical approach in many domains as they reduce information in a way that can identify otherwise hidden patterns and structures. Instead of looking at the strength of trade connections based on value and quantity of trade goods, we look at the structure of the maritime network that will deliver these goods.\nThe enriched port visits data that can build ship schedules or understand shipping delays can also be used to find patterns of maritime connectivity.\nNodes, aggregated to countries for convenience, are connected by ship voyages. Direct connections indicate adjacent ports in the visit sequence while indirect connections are either ports further along the schedule or connected by common ports on different routes.\n\nThe relative strength of adjacent connections vs. connections further away is a handy way to split the network into communities. Like social networks, communities of strongly connected nodes are more likely to share information and cooperate, or in more dire circumstances, fail together.\n\n\nDisclaimer\nThe contents and figures in this post are not official outputs from the Ministry of Transport. They are research-oriented exploratory analyses intended as demonstrations of approaches and techniques relevant to public sector data science."
  },
  {
    "objectID": "projects/shipping-delays/index.html",
    "href": "projects/shipping-delays/index.html",
    "title": "Voyage distributions and shipping Delays",
    "section": "",
    "text": "Container shipping has been left to the market by New Zealand policymakers - expecting competitive market forces to converge on optimal routes given maritime network structure, demand and port capacity / attractiveness. However, a cascading set of labourforce disruptions and stringent COVID-19 policies have resulted in significant delays to container ship voyages since the onset of the COVID-19 pandemic.\nGiven New Zealand’s geographical position as well as its wide set of trading partners, we see considerable variation in both the absolute value as well as the shape of voyage durations. Longer routes like Australia-Eastern Asia-New Zealand and Latin America-New Zealand-Northern America have the most stable shape (symmetric Gaussian/Lorentzian) while voyages on the considerably closer Trans-Tasman route (Australia-New Zealand) are heavily skewed. A long tail is also seen for voyages on the important Australia-New Zealand-Southeastern Asia route - connecting New Zealand to key trans-shipment hubs like Singapore and Port Klang.\n\nWe can zoom into the temporal patterns for voyages on the two problematic routes. For Trans-Tasman voyages, delays have no clear pattern. For voyages to Southeastern Asia, voyage durations peaked between April - June 2021 before decreasing again.\n\n\nDisclaimer\nThe contents and figures in this post are not official outputs from the Ministry of Transport. They are research-oriented exploratory analyses intended as demonstrations of approaches and techniques relevant to public sector data science."
  },
  {
    "objectID": "projects/legacy-code-to-pipeline/index.html",
    "href": "projects/legacy-code-to-pipeline/index.html",
    "title": "From Legacy Code to Pipeline",
    "section": "",
    "text": "Legacy code is a burden for any developer. Depending on the state of the code, maintenance and improvements are not necessarily simple. From my experience in the public sector, legacy code for analyses and ETL (Extract, Transform, Load) are neither written by developers who were conversant in modern software development practices (like version control, automation, unit testing etc.) nor is the code particularly well-documented. As a result, maintenance is time consuming, manual and unwieldy. As part of my recent role in the government, inherited code bases became a nightmare to maintain and use until I discovered the delightful trifecta of jupyter, saspy and exchangelib.\nI’ve given a couple of talks on this topic: a long version with only slides here and a short version with slides and a 5 minute video here. I’ve also written an introductory blog post here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "COVID-19 impacts on economic indicators\n\n\n\ncovid-19\n\n\npandemic\n\n\neconomy\n\n\nindicators\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTravel Activity Patterns\n\n\n\ndata science\n\n\ntravel\n\n\ntransport\n\n\nbehaviour analysis\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaritime Connectivity\n\n\n\ndata science\n\n\ntransport\n\n\nnetwork analysis\n\n\nmaritime\n\n\nshipping\n\n\nais\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nJan 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLong tail behaviours\n\n\n\ndata science\n\n\nmarketing\n\n\nretail\n\n\nbehaviour analysis\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelling the real world with hierarchical models\n\n\n\ndata science\n\n\nretail\n\n\nbayesian\n\n\nmodelling\n\n\nstatistics\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Legacy Code to Pipeline\n\n\n\ndata science\n\n\nreproducibility\n\n\npublic sector\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nJan 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning customer preference: from size to fit\n\n\n\ndata science\n\n\nretail\n\n\nexperiments\n\n\nbehaviour analysis\n\n\nbayesian\n\n\nmodelling\n\n\nstatistics\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nDec 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural experiments for online behaviours\n\n\n\ndata science\n\n\nmarketing\n\n\nretail\n\n\nexperiments\n\n\nnetwork analysis\n\n\nbehaviour analysis\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nShipping Schedules\n\n\n\ndata science\n\n\nshipping\n\n\nmaritime\n\n\ncovid-19\n\n\npandemic\n\n\nais\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoyage distributions and shipping Delays\n\n\n\ndata science\n\n\nmaritime\n\n\ncovid-19\n\n\npandemic\n\n\nais\n\n\nshipping\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting customer behaviour: returns\n\n\n\ndata science\n\n\nmarketing\n\n\nretail\n\n\nbehaviour analysis\n\n\n\n\n\n\n\nShrividya Ravi\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Shrividya Ravi",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \nWelcome to my website!\nI am a data scientist working to push the analytics capabilities of public sector work in transport. My current work at the Ministry of Transport involves new modelling paradigms for transport policy (including agent based modelling and bayesian models), comprehensive re-design of legacy workflows to reproducible analytical pipelines (RAP) and building towards agile and collaborative ways of working for public sector analysts.\nYou can find out more about projects, explore my articles, read my blog, or reach out by e-mail."
  }
]