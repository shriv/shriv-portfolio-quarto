[
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Build analyses as reproducible analytical pipelines\n\n\n\n\n\n\ndata science\n\n\ngood practice\n\n\nreproducibility\n\n\nnew zealand\n\n\npublic sector\n\n\n\nMake it easy to run\n\n\n\n\n\nFeb 9, 2024\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses as packages\n\n\n\n\n\n\ndata science\n\n\ngood practice\n\n\nR\n\n\nreproducibility\n\n\nnew zealand\n\n\npublic sector\n\n\n\nFile structure and clarity for analyses\n\n\n\n\n\nFeb 9, 2024\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nOpen source and version control\n\n\n\n\n\n\ndata science\n\n\nproduct\n\n\ncommunication\n\n\npublic sector\n\n\n\nThe foundation of analytics\n\n\n\n\n\nFeb 9, 2024\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nData product perspective\n\n\n\n\n\n\ndata science\n\n\nproduct\n\n\ncommunication\n\n\npublic sector\n\n\n\nTechnical analyses tailored to stakeholders\n\n\n\n\n\nFeb 9, 2024\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nDesign documents before coding\n\n\n\n\n\n\ndata science\n\n\ngood practice\n\n\nnew zealand\n\n\npublic sector\n\n\n\nSave time and headspace in data science projects\n\n\n\n\n\nFeb 9, 2024\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nWriting practical documentation\n\n\n\n\n\n\ndata science\n\n\ngood practice\n\n\ndocumentation\n\n\ncommunication\n\n\nnew zealand\n\n\npublic sector\n\n\n\nDocumentation as communication not drudgery\n\n\n\n\n\nFeb 9, 2024\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nLearning from challenging portfolios\n\n\n\n\n\n\ndata science\n\n\ntransport\n\n\nnew zealand\n\n\npublic sector\n\n\n\nLessons from 4 years at the Ministry of Transport\n\n\n\n\n\nDec 27, 2023\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nEnriching independent data with industry-relevant concepts\n\n\n\n\n\n\ndata science\n\n\nmaritime\n\n\nshipping\n\n\nais\n\n\nnew zealand\n\n\nnetwork analysis\n\n\n\nAdding container shipping concepts to AIS data\n\n\n\n\n\nJan 25, 2023\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nRAPping in the public sector\n\n\n\n\n\n\ndata science\n\n\nreproducibility\n\n\npublic sector\n\n\npipelines\n\n\n\nBuilding reproducible analytical pipelines (RAP) around SAS with Python\n\n\n\n\n\nNov 10, 2021\n\n\nShrividya Ravi\n\n\n\n\n\n\n\n\n\n\n\n\nModelling - a brief reflection\n\n\n\n\n\n\ndata science\n\n\nmodelling\n\n\nstatistics\n\n\nbayesian\n\n\n\nThoughts on why statistical modelling is useful\n\n\n\n\n\nDec 23, 2020\n\n\nShrividya Ravi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/design-documents/index.html",
    "href": "articles/design-documents/index.html",
    "title": "Design documents before coding",
    "section": "",
    "text": "Data analysts write code. Often quite a lot. We are effectively software developers regardless of our job titles. This fact doesn’t need to put off any analysts since a developer’s job is to solve problems rather than write code [1], much the same as a number of other domains that include data analysts, data scientists, scientists etc. This means that good practices across seemingly disparate domains can cross-pollinate because they share the same ethos. I’ve already written about improving reproducibility, a key practice among scientists, with the suggestion for analysts to build analyses as reproducible analytical pipelines. I will now do the same for borrowing design considerations from software development for data analysis.\nFor a long time, I considered myself separate to software developers because I didn’t equate the analysis work I did to feature design, development and testing. However, the underlying cycle of design, development and testing is fundamental to solving any problem for users via a product. Even if the product is just for ourselves or a single business stakeholder, a data product perspective can be very beneficial to data analysts.\nBefore the development work to structure data analysis as a package and build analyses as reproducible analytical pipelines even begin, it is useful to reflect upon the design. Every analysis makes assumptions about the data required, chooses a particular methodology and presents the output in a certain way. All these choices are part of the analysis design. These design choices are a core part of the actual analysis and must be incorporated into the review process of any product that is delivered to a user. A design document facilitates this vital part in a consistent manner.\nDesign documents describe implementation strategy and design decisions with robust discussion of trade-offs [1]. They start with why and what questions for elaborating context, requirements and constraints [2]. These questions place the analysis in the frame of the request or need which predicates the subsequent decisions around data, method and presentation.\nThe core of a design document suggests solutions to the problem before justifying a particular solution [1]. This should be reasonably easy to write as there is always an alternative, even if it is trivial. My personal experience of writing about alternatives has challenged the solution I honed in on too quickly. Sometimes, the rationale came down to ease and speed but in others, features from the alternatives improved my preferred solution.\nDescribing the preferred solution should cover most of the components of data science methodology: data, techniques, validation and communication [2]. Depending on preference, alternatives can be listed for each component. However, I have found more alternatives exist for techniques than the other components.\nIn an organisation or data team where projects are pursued based on priority, design documents can be instrumental in proving value before building the data product [3].\nHowever, the greatest value is that design documents augment organisational knowledge [1]. This is applies as well to a large organisation where similar skills are spread across multiple teams to a small organisation with a mix of experience.\nCapturing the design considerations and discussion in a document is a boon for future team members who might find themselves needing to work with code that was designed and written by someone long one. The design document is a benevolent “Ghost of Analyst Past” gently informing and guiding the new analyst.\nThe threshold of when an analysis is too simple for a design phase can be difficult to set. A simple rubric is to write a design document for complex and ambiguous problems [1]. For simpler problems, I have found it sufficient to detail the methodology that will be implemented into the Merge Request (Gitlab equivalent of the Pull Request) description.\nDesign documents exemplify the valuable perspective that writing documents is expensive, but cheap [2]. Time and effort is expended in writing and reviewing the document but this effort pays off in the long run. Consensus is set upfront and kinks in approach are ironed out at the outset allowing the project to progress smoothly."
  },
  {
    "objectID": "articles/design-documents/index.html#credit",
    "href": "articles/design-documents/index.html#credit",
    "title": "Design documents before coding",
    "section": "Credit",
    "text": "Credit\nPhoto by Firmbee.com on Unsplash"
  },
  {
    "objectID": "articles/data-product-perspective/index.html",
    "href": "articles/data-product-perspective/index.html",
    "title": "Data product perspective",
    "section": "",
    "text": "My experience in a small public sector organisation has largely observed analyses outputs cobbled together into an email with a paragraph or two of explanation and then followed by plenty of back and forth to get the message or caveats well understood. This approach has many disadvantages: from asset confusion (which version has the right changes) to amnesia on both sides on the original request and context as emails exchanges request change upon change. These experiences are not a standard but I suspect they happen more often than not leading to frustrations for both the technical analysts as well as the business stakeholders.\nFollowing a series of such interactions and observing others, I realised that core to the frustrations was poor definition around the deliverable provided to the end user. Figures and tables, while seemingly simplistic require explanation and narrative to make sense to the non-technical person and a short paragraph in an email may not be sufficient for understanding. Furthermore, some analyses are better provided with interactivity so that the user can explore and take what they need.\nI also found that we were all developing analyses as a collection of scripts with some instructions to run them. Even if analysts [[build analyses as reproducible analytical pipelines]] analyses often remained a technical obscurity without a useful output tailored for the end users.\nThese thoughts coalesced into the idea that an analysis is best developed as a data product. Shaping a piece of work as a product makes it tractable. We can summarise a product with purpose, audience and value. A data product can be a pdf report, a web output, a simple interactive dashboard or a web app. It is simply a tangible asset for a piece of work that can be designed and shaped by user needs and preferences.\nI’ve chosen to use a generic term of data product rather than something like analysis report. This is because there are many instances where a simple dashboard with interactive charts or perhaps a web app are the best data products to package up an analysis. However, the majority of analysis work fits well as a “report” format, especially with the wide flexibility offered by Quarto.\n\n\n\nScreenshot of Quarto features from website.\n\n\nQuarto documents (be they pdf outputs, websites or dashboards) offer the best balance for the technical analyst. The executable document contains code and links to data, useful to the technical person, while the rendered output is designed for the target audience. I’ll go through a brief transport example to highlight the use and value of data products.\nThroughout the COVID-19 pandemic and its lingering aftermath there were many questions around maritime resilience and connectivity. For transport policymakers questions abound on the nature and type of shipping lanes connecting New Zealand and the Pacific islands to global trading partners. There were specific questions but also a desire for a holistic understanding. Given this need a web-book was the best data product: an introduction provided the wider view while individual chapters presented detailed analyses on the requested topics. The website was hosted on Gitlab Pages and made available to users as an URL.\n\n\n\nAn interactive report data product developed for supply chain insights relevant to transport policymakers.\n\n\nA web-book data product functions like a reference document with clear global navigation for users to jump into the area of most interest or pertaining to their questions. This was partly there were several users and related but distinctly different analyses.\nThis data product (and others like it) were actually constructed from user stories. This term hails from Agile methodologies but can be used for data analyses without the work management aspects. This is because at their essence user stories underlie the product [1] regardless of the type of product. User stories become features for a data product i.e. data analyses. A good user story includes the need (the question) and the context of the user giving the analyst sufficient information to best provide information and insights.\nThe best products evolve by iteration; through presentation to and feedback from the end users. Unlike an email chain where people can get lost finding the artefacts with the changes they requested, a data product especially a web-based one, can be iterated upon based on feedback with the improved version available at the same location as the old. For complex projects, building cheap data product prototypes with made-up data can be iterated with the users to ensure the full build is only handled once the core use and functionality is agreed upon [2].\nWhile every analysis request or project I have worked on in the last couple of years has an associated data product, the process has been far from smooth and with persisting points of friction. For example, the lack of a general tool for detailed feedback has been particularly problematic.\nSince data products are fundamentally technical assets, they are hosted on a version control system (VCS). Gitlab in our case. The rendered output (be it web app or plain HTML) are provided at an accessible URL to end users. These outputs can be explored and feedback is usually solicited in a face to face (in person or virtual) meeting. However, the tools we have chosen don’t have the equivalent of Microsoft’s “Track Changes”. This limits the data products to ones that will be edited by the technical analysts alone. We are unable to collaborate on the text of the data products in a manner that still allows for technical development with the appropriate tools.\nWhile the friction of collaboration is galling, the model works well for most analyses. To recap, a data product is a tangible asset of data analysis. Depending on use and user, it can be a pdf report, website, web app or interactive dashboard. Work is best done by first collecting the requests as user stories which are developed as analyses and consolidated into an appropriate data product, noting to write design documents before coding! The data product is then iterated upon based on user feedback. To avoid search fatigue and asset confusion, data products are best hosted at a centralised location.\n\n\n\n\nReferences\n\n[1] M. Britsch, “Product Management Playbook v1.0.” Accessed: Jan. 17, 2024. [Online]. Available: https://drive.google.com/file/d/1YAzjWwyi1Yh2csqmny6YmnqUg5Y9ihcX/view?usp=sharing&usp=embed_facebook\n\n\n[2] A. Viana, “Built It Once & Build It Right: Prototyping for Data Teams.” https://www.getdbt.com/coalesce-2021/prototyping-for-data-teams/ (accessed Oct. 21, 2023)."
  },
  {
    "objectID": "articles/open-source-vcs/index.html",
    "href": "articles/open-source-vcs/index.html",
    "title": "Open source and version control",
    "section": "",
    "text": "Tracking. Collaboration. Innovation. Transfer. These virtues of software development with open source tools and version control are valued so much that the tools are ubiquitous across most software industries. However, they have not been as prominent in public sector analytics despite data analysts being no different to a developer: solving problems with code.\nProviders like SAS have a historical precedence of working closely with public sector agencies with considerable support. They also inserted themselves at the open end of the HR funnel by subsidising university licences across courses that train future analysts. This cleverly engineered dependence cycle has led to the dominance of proprietary tools. For a long time open source has also been regarded with a suspicious eye, especially by those in influential positions of organisational security. Reasonably so, since the messy, melting pot of open source ecosystems are not as tractable from a security standpoint compared to a product that will be installed on a single machine by IT personnel.\nBut the tide is changing. More teams are finding themselves up against the limits of tools like SAS compared to the vast ecosystems in R, Python, Julia etc. Proprietary tools are also harder to set up or deploy on the cloud; an area of increasing investment for many public sector organisations.\nOpen source and version control underpin the ethos to build analyses as reproducible analytical pipelines. Good practices around reproducibility and improving methodologies are impossible without projects that can be transferred or a space for collaboration.\nA centralised version control system backed up with a service like Github or Gitlab or even self-hosted makes review extremely easy. The reviewer can pull the entire project and add their comments, suggestions and changes making the work more rigorous and robust. A second reviewer would add to the rigour and quality.\n\nAnalysts can also collaborate easily with version control either in sequence but ensuring that the original work was still protected or in tandem. In these modes of collaboration, the messy part of working together on the same project are immeasurably facilitated by version control taking care of branching and merging where required. The analysts just need to focus on their work rather than admin\n\nFor many analysts, version control and open source are no-brainers. However, I have observed several instances of deep reluctance to set up such a tool in organisations. Sadly, the even greater reluctance is that of the analysts themselves. Too many of us have become ingrained in ways that produce reasonable quality work on our terms. When we have arrived at a sweet spot of tools and approaches, what is the point in spending agonising hours (perhaps even weeks and months) on new tools and processes? The real challenge to our status quo is the fundamental recognition that the work is bigger than us. While we all hold a strong attachment to our “project babies”, they will only grow into functional beings with the help of the village i.e. our immediate and wider team nurturing their strengths, fixing their bugs and coaxing better performance."
  },
  {
    "objectID": "articles/analysis-as-rap/index.html",
    "href": "articles/analysis-as-rap/index.html",
    "title": "Build analyses as reproducible analytical pipelines",
    "section": "",
    "text": "“Pipelines over people” is a time-saving paradigm in analytics. Any process managed by individuals tends to accrue cumbersome, undocumented manual steps though there are instances where ad-hoc process could be reasonable. The landscape of data science (visualised below) summarises processes along the dimension of analytical complexity vs. frequency. Academic processes fall into the top left corner where more time is spent on state-of-the-art algorithms and approaches than being able to ship the analysis to someone else or being able to repeat the analysis in future. The tide has turned in recent years however, with considerable efforts in academia to fight the “reproducibility crisis” by establishing robust and reproducible experimental methods as well as data analysis.\n\n\n\nLandscape of data science. Adapted from a presentation by Tom Beard.\n\n\nAnalytics in the public sector, on the other hand, occurs at a regular cadence of simple to complex algorithms. Yet, projects are often handled as delicate artisanal processes rather than pragmatic pipelines. Such processes fall neatly into the paradigm of reproducible data science and can be re-engineered into a reproducible analytical pipeline (RAP).\nRAP is a clever acronym coined by databods in theUK government that emphasises good software carpentry relevant to analysts. While reproducible pipelines are not a substitute for data infrastructure, they are an accessible first step in the journey to better data engineering. Too often analysts get stuck with poor process and the answer is never a moonshot to modern data tools like dbt or data lakes.\nOne RAP approach is a “one command” or “one click” pipeline. Re-engineering a process such that someone else (a team member or even future you) can run the whole pipeline end-to-end with one command or one click just requires an end-to-end reproducibility-focused project design pattern. A Jupyter notebook run within a conda environment is a common “one click” pattern for Python users (see here and here). A more flexible pattern is a make shortcut that runs Docker applications for “one command”.\nThe make + Docker RAP pattern runs code as applications inside a Docker container while the make “one command” strings together a linear or DAG (directed acyclic graph) pipeline. Code can be written in any open source programming language and processes can even contain components written in different languages where each component runs in its own Docker container. For example, several of my ETL processes have an “extract” written in Python and a “transform” written in R while a bash script “loads” data to a shared location.\nThe project structure of a make + Docker RAP (R project as the base example) needs a minimum of three additional files in the root directory. A Python equivalent would replace the renv.lock file with an environment.yml, requirements.txt or poetry.lock depending on the Python environment tool being used.\n\nMakefile - contains shortcuts for common executions e.g. make run_etl which would run a data processing pipeline of Extract, Transform and Load.\nDockerfile - contains explicit system requirements to run the code.\nrenv.lock - contains language specific dependencies.\n\nDepending on the project a small number of additional files can prove useful.\n\n.Rprofile - contains useful instructions for R. For example, a conditional that instructs renv to download binaries from RSPM when the OS is Windows or Linux and to use CRAN for OSX. This allows for fast environment restores where possible.\n.Renviron - contains environment variables like AWS secret keys, Gitlab tokens to download private packages etc.\n\nconfig.yml - contains modifiable parameters for analysis.\n\nRunning a RAP through a make shortcut can be as easy as make run_etl, a common shortcut for many of my projects. The run_etl command just executes steps in sequence though make can also handle complex DAGs. In this example, get_ecr_images first downloads the Docker images from a private container registry while the run_docker step runs a Docker container before loading the data to a shared location via a bash script. The Makefile contains shortcuts to build_images and push_images_to_ecr.\nrun_etl: get_ecr_images run_docker copy_to_s3\nA new analyst (or a future you with a new laptop) only needs Docker and a bash terminal to run the process. In this case, there is no dependency on historical data but in some instances the ETL needs to combine new data with old and the run_etl step can also include a data download from a shared location.\nThe make + Docker combination that can also be used to set up a development environment. For example, connecting to including an IDE for interactive checks and tests prior to running the pipeline. This is often helpful for insights reports where make + Docker can just provide a reproducible environment for the report. More on this in an upcoming post on reproducible development environments.\nOne of the drawbacks of the make + Docker RAP is the locking of potentially too old dependencies within renv.lock which takes a snapshot of the local system settings e.g installed version of R and compatible dependencies rather than creating an environment current to the time the project was set up. So far, Docker and the various R package repositories have maintained pretty good backwards compatibility but in case this changes, analysts can safeguard for future reproducibility with images backed up in a container registry.\nTo conclude, make + Docker is a fantastic paradigm for porting analyses across time and people. For anyone interested in delving into further detail on the topics on reproducible data science (especially with R) check out Bruno Rodrigues’s excellent open book on reproducible analytical pipelines."
  },
  {
    "objectID": "articles/challenging-lessons/index.html",
    "href": "articles/challenging-lessons/index.html",
    "title": "Learning from challenging portfolios",
    "section": "",
    "text": "Within three months of starting at the Ministry of Transport (MOT) I was handed over a large portfolio of reporting (from a colleague who was leaving) that was entirely in SAS, Excel and Tableau. To say I was shocked and stressed is an under-statement. I had negligible skills in Excel and no knowledge of SAS. There was one silver lining: no requirement to continue with SAS. In fact, it was the opposite.\nThe year I joined marked the start of a concerted move away from proprietary tools like SAS and non-code tools like Excel. I just needed to re-do processes away from the old paradigm to a new, open source one. But, there was little other directive. Working through this challenge has taught me technical skills I was sorely lacking, and perspectives that have expanded my thinking. Cesar Millan’s quote “You don’t always get the dog you want, but you get the dog that you need” works just as well for a dog of a portfolio.\nAfter 4 years in the Analytics & Modelling team at MOT, I have switched to a new team. This transition is bittersweet. I’m excited to work on transport simulations, especially around urban challenges, but I leave behind memorable pieces of work with digital traces of my blood, sweat and tears (not to mention lurking bugs). To honour these hard-earned lessons, I’m committing them as a series of short posts.\nI believe these posts have an audience of greater than one (i.e. other than me). Data scientists in the public sector, especially in policy-focused agencies, battle very different problems to private sector and the relevant technical skills are typically not deployment of realtime ML models or building better user experience with LLMs. The useful skills suite, summarised below, is that of an academic - working mostly by themselves or in a small team of domain experts.\nAlways write design documents before coding with a data product perspective. Every project needs to have a clear scope and approach that should be vetted (by both technical reviewers and business customers) before the actual work begins. This can save an enormous amount of time and headspace. When working on a data science project good software carpentry will save a lot of headache during review or running code in the future. Key lessons here are to structure your analysis as a package and build analyses as reproducible analytical pipelines. Of course, all these aspects are built on the solid foundation of open source and version control and enabled by writing practical documentation."
  },
  {
    "objectID": "articles/challenging-lessons/index.html#credit",
    "href": "articles/challenging-lessons/index.html#credit",
    "title": "Learning from challenging portfolios",
    "section": "Credit",
    "text": "Credit\nPhoto by Towfiqu barbhuiya on Unsplash"
  },
  {
    "objectID": "articles/modelling/index.html",
    "href": "articles/modelling/index.html",
    "title": "Modelling - a brief reflection",
    "section": "",
    "text": "While working on the geospatial analysis of walkability, I realised that modelling requires its own introduction and it’s worth taking a high-level persepective into why modelling is useful. I hope to make the case that approximating reality with models allows us to dredge up some useful insights.\nApplications of this post can be seen in the final post of walkability, where we will look at: - The accessibility characteristics of a suburb - Suburban characteristics don’t fit our model approximations. And how we can update our model to better reflect reality.\n\n\nThe best reason for trying our hand at statistical modelling comes from the entertaining and brilliant pedagogue: Ben Lambert.\n\nIn life, noise obfuscates signal. What we see often appears as an incoherent mess that lacks any appearance of logic.\n\n\nStatistical inference is the logical framework we can use to trial our beliefs about the noisy world against data. We formalise our beliefs in models of probability.\n\n\n\nA Student’s Guide to Bayesian Statistics, Ben Lambert (p 17)\n\n\n\nIn his book, Lambert goes on to elaborate the gains acheived from employing a Bayesian approach to statistical inference. Our analysis into accessibility by suburb doesn’t explicitly benefit from a Bayesian approach but I’ve chosen to use it anayway since I’m now completely avowed to The Bayesian Way (Bayes-do?).\n\n\n\nThe core component of statistical inference is a statistical model - often shortened to just model. Common models formalise the data generation process by quantifying the relationhip between inputs and outcomes. For example, linear regression models quanitfy the relationship between a set of user-defined inputs and the possible outcomes given those inputs.\nThe model we’re using in this post is much simpler: we’re considering the probability space of the outcomes - with a particular interest in summary statistics like the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\). As we’ll see, we choose a particular mathematical form to represent the probability space of average (\\(\\mu\\)) and heterogeneity (\\(\\sigma\\)) of accessibility within a suburb.\n\n\n\nIt’s worth noting that not all data-driven questions benefit from statistical modelling. Models can be complicated and difficult to explain to others - even technically-oriented peers. In view of this, some data evangelists advocate a simpler analysis process. Kat Greenbrook highlights how the modelling aspect can be left out for many business analytics questions.\n\n\n\n\n\n\n\n\nData Science cycle\nData Analysis cycle\n“Modelling Silo”\n\n\n\n\n\n\n\n\n\nActions from model output / insights\nActions directly from exploratory analysis\nPointless modelling. No pertinent question and no actions\n\n\n\n\n\nImages © Kat Greenbrook\n\n\n\nData anlysis alone is powerful; exploratory analyses unearth useful insights that can be followed through with business actions. As people who harness data for a purpose, we must constantly evaluate whether the extra complexity of the model layer is adding significant utility and insight.\n\nModelling should never be undertaken if there is not a clear use case for the output.\n\n\n\nKat Greenbrook\n\n\n\nAs someone who has frequently lunged into modelling without a cause, I can attest to the pervasive culture of the ‘Modelling Silo’ in Data Science. This ‘cycle’ is wholly disconnected to pertinent questions and, any useful actionable output.\n\n\n\nNow that we have been cautioned to think before we model, we can identify how models can help better understand playground accessibility in Wellington.\nIn the previous post, we ended with heatmaps of accessibility - defined as total travel time. The heatmaps conveyed a holistic picture of areas with worse accessibility due to the hilly topography. However, we couldn’t pick out any details from the overview. For example, we might care about how our specific neighbourhood compares to another, or our neighbourhood vs. the average for the city.\nComparisons can be done with single point values alone. But, robust comparisons rely on statistical inference - the most classic being the t-test for comparing two means. In the walkability modelling post, we will see how we can robustly compare suburbs using a Bayesian statistical model.\n\n\n\nAdding a model for comparing suburbs has further utility - it can be used for explicit or qualitative decision making. Explicit decisions are appropriate in a business context since executives want to tl;dr the best option. Since this series is more focused on exploration, we’ll be building a qualitative picture guided by metrics and analyses.\n\nCan we summarise the playground accessibility characteristic for a given suburb?\n\nThis question can help understand how “family-friendly” a particular suburb is. Young families could compare the suburb accessibility characteristics to help make the decision for a move or, evaluate whether the suburb is right for their lifestyle.\nFrom this question and potential use, we can desgin the model and outputs for an intuitive comparative analysis. The final model allows for two levels of qualitiative comparison: (1) comparing a single suburb to the city average or, (2) comparing any two suburbs together.\n\n\n\nSince models approximate reality, the difference between the model and reality can also add valuable insight.\n\nWhich suburbs don’t follow the approximation set by the model? Can we use our domain knowledge to understand why?\n\nIn this scenario, mismatch between the data (‘reality’) and the model can help us understand the nature of suburbs better; and use this understanding to update our model for a better representation of reality."
  },
  {
    "objectID": "articles/modelling/index.html#introduction",
    "href": "articles/modelling/index.html#introduction",
    "title": "Modelling - a brief reflection",
    "section": "",
    "text": "While working on the geospatial analysis of walkability, I realised that modelling requires its own introduction and it’s worth taking a high-level persepective into why modelling is useful. I hope to make the case that approximating reality with models allows us to dredge up some useful insights.\nApplications of this post can be seen in the final post of walkability, where we will look at: - The accessibility characteristics of a suburb - Suburban characteristics don’t fit our model approximations. And how we can update our model to better reflect reality.\n\n\nThe best reason for trying our hand at statistical modelling comes from the entertaining and brilliant pedagogue: Ben Lambert.\n\nIn life, noise obfuscates signal. What we see often appears as an incoherent mess that lacks any appearance of logic.\n\n\nStatistical inference is the logical framework we can use to trial our beliefs about the noisy world against data. We formalise our beliefs in models of probability.\n\n\n\nA Student’s Guide to Bayesian Statistics, Ben Lambert (p 17)\n\n\n\nIn his book, Lambert goes on to elaborate the gains acheived from employing a Bayesian approach to statistical inference. Our analysis into accessibility by suburb doesn’t explicitly benefit from a Bayesian approach but I’ve chosen to use it anayway since I’m now completely avowed to The Bayesian Way (Bayes-do?).\n\n\n\nThe core component of statistical inference is a statistical model - often shortened to just model. Common models formalise the data generation process by quantifying the relationhip between inputs and outcomes. For example, linear regression models quanitfy the relationship between a set of user-defined inputs and the possible outcomes given those inputs.\nThe model we’re using in this post is much simpler: we’re considering the probability space of the outcomes - with a particular interest in summary statistics like the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\). As we’ll see, we choose a particular mathematical form to represent the probability space of average (\\(\\mu\\)) and heterogeneity (\\(\\sigma\\)) of accessibility within a suburb.\n\n\n\nIt’s worth noting that not all data-driven questions benefit from statistical modelling. Models can be complicated and difficult to explain to others - even technically-oriented peers. In view of this, some data evangelists advocate a simpler analysis process. Kat Greenbrook highlights how the modelling aspect can be left out for many business analytics questions.\n\n\n\n\n\n\n\n\nData Science cycle\nData Analysis cycle\n“Modelling Silo”\n\n\n\n\n\n\n\n\n\nActions from model output / insights\nActions directly from exploratory analysis\nPointless modelling. No pertinent question and no actions\n\n\n\n\n\nImages © Kat Greenbrook\n\n\n\nData anlysis alone is powerful; exploratory analyses unearth useful insights that can be followed through with business actions. As people who harness data for a purpose, we must constantly evaluate whether the extra complexity of the model layer is adding significant utility and insight.\n\nModelling should never be undertaken if there is not a clear use case for the output.\n\n\n\nKat Greenbrook\n\n\n\nAs someone who has frequently lunged into modelling without a cause, I can attest to the pervasive culture of the ‘Modelling Silo’ in Data Science. This ‘cycle’ is wholly disconnected to pertinent questions and, any useful actionable output.\n\n\n\nNow that we have been cautioned to think before we model, we can identify how models can help better understand playground accessibility in Wellington.\nIn the previous post, we ended with heatmaps of accessibility - defined as total travel time. The heatmaps conveyed a holistic picture of areas with worse accessibility due to the hilly topography. However, we couldn’t pick out any details from the overview. For example, we might care about how our specific neighbourhood compares to another, or our neighbourhood vs. the average for the city.\nComparisons can be done with single point values alone. But, robust comparisons rely on statistical inference - the most classic being the t-test for comparing two means. In the walkability modelling post, we will see how we can robustly compare suburbs using a Bayesian statistical model.\n\n\n\nAdding a model for comparing suburbs has further utility - it can be used for explicit or qualitative decision making. Explicit decisions are appropriate in a business context since executives want to tl;dr the best option. Since this series is more focused on exploration, we’ll be building a qualitative picture guided by metrics and analyses.\n\nCan we summarise the playground accessibility characteristic for a given suburb?\n\nThis question can help understand how “family-friendly” a particular suburb is. Young families could compare the suburb accessibility characteristics to help make the decision for a move or, evaluate whether the suburb is right for their lifestyle.\nFrom this question and potential use, we can desgin the model and outputs for an intuitive comparative analysis. The final model allows for two levels of qualitiative comparison: (1) comparing a single suburb to the city average or, (2) comparing any two suburbs together.\n\n\n\nSince models approximate reality, the difference between the model and reality can also add valuable insight.\n\nWhich suburbs don’t follow the approximation set by the model? Can we use our domain knowledge to understand why?\n\nIn this scenario, mismatch between the data (‘reality’) and the model can help us understand the nature of suburbs better; and use this understanding to update our model for a better representation of reality."
  },
  {
    "objectID": "articles/analysis-as-package/index.html",
    "href": "articles/analysis-as-package/index.html",
    "title": "Analyses as packages",
    "section": "",
    "text": "R emphasises a functional style of decomposing a big problem into smaller pieces, then solving each piece with a function or combination of functions [1]. Functions can be compiled into package for ease of use across multiple projects or shipped to other users. The package structure enables thorough documentation (functions with Roxygen and usage with vignettes) and includes tests that can be run every time a change is pushed. These virtues make packages the right project structure for the three common data science outputs: analyses, tools and applications. In other words, everything is a package [2].\nFrom a more pragmatic perspective the R package structure is consistent enough to be a cookie cutter structure for data analyses [3]. Project structures anchor development and offer a consistent interface for other users. Python has several options for cookie cutter structure due to the wide range of coding styles while the structure of an R package is powerful enough for common outputs like a web app, report or just a basic analysis.\nDenis Gontchorov provides an excellent rundown of how the package structure can be used for the most common tasks faced by data professionals: exploratory data analysis (EDA) [3]. Let’s summarise the key folders and how they facilitate common tasks:\nMore information on how to work with R packages can be found in the online book on R packages [4]. I recommend the chapter, “The Whole Game”, for a step-by-step tutorial for creating a basic package.\nThe R package structures code in order to build analyses as reproducible analytical pipelines. In all my years of working as a data scientist, simple approaches like this have yielded the greatest dividends in productivity and innovation. Reducing the overhead of thinking of project structure leaves headspace for innovative analyses and shipping better quality code."
  },
  {
    "objectID": "articles/analysis-as-package/index.html#credit",
    "href": "articles/analysis-as-package/index.html#credit",
    "title": "Analyses as packages",
    "section": "Credit",
    "text": "Credit\nPhoto by Mediamodifier on Unsplash"
  },
  {
    "objectID": "articles/rap-public-sector/index.html",
    "href": "articles/rap-public-sector/index.html",
    "title": "RAPping in the public sector",
    "section": "",
    "text": "Public sector, indeed even private sector, analytics are rife with silos and people-driven pipelines. Instead of building processes with minimal manual interference, pipelines are ususally a mash of the metaphorical ductape and frenetic manual steps resulting in blood, sweat and tears for any analyst who subsequently picks up the work.\nManual processes need to be overseen: to start, pause, stop, make changes, perform checks etc. This overload of analyst headspace can have costly outcomes due to inevitable human error. Fortunately, most of these steps can be automated allowing the analyst to dedicate their skills to using the data and providing business-relevant value.\n\n\n\n\n\ngraph TD\ndata --&gt; a[Put in Excel]\na --&gt; b[Run manual calculations]\nb --&gt; c1[Excel sheet &lt;br&gt; with &lt;br&gt; manual checks]\nb --&gt; c[Copy output &lt;br&gt; to &lt;br&gt; Excel / Word]\nc --&gt; e[Review]\ne --&gt; |Changes needed| b\ne  --&gt; |Changes needed| data"
  },
  {
    "objectID": "articles/rap-public-sector/index.html#the-ubiquity-of-bad-processes",
    "href": "articles/rap-public-sector/index.html#the-ubiquity-of-bad-processes",
    "title": "RAPping in the public sector",
    "section": "",
    "text": "Public sector, indeed even private sector, analytics are rife with silos and people-driven pipelines. Instead of building processes with minimal manual interference, pipelines are ususally a mash of the metaphorical ductape and frenetic manual steps resulting in blood, sweat and tears for any analyst who subsequently picks up the work.\nManual processes need to be overseen: to start, pause, stop, make changes, perform checks etc. This overload of analyst headspace can have costly outcomes due to inevitable human error. Fortunately, most of these steps can be automated allowing the analyst to dedicate their skills to using the data and providing business-relevant value.\n\n\n\n\n\ngraph TD\ndata --&gt; a[Put in Excel]\na --&gt; b[Run manual calculations]\nb --&gt; c1[Excel sheet &lt;br&gt; with &lt;br&gt; manual checks]\nb --&gt; c[Copy output &lt;br&gt; to &lt;br&gt; Excel / Word]\nc --&gt; e[Review]\ne --&gt; |Changes needed| b\ne  --&gt; |Changes needed| data"
  },
  {
    "objectID": "articles/rap-public-sector/index.html#rap-to-overcome-bad-processes",
    "href": "articles/rap-public-sector/index.html#rap-to-overcome-bad-processes",
    "title": "RAPping in the public sector",
    "section": "RAP to overcome bad processes",
    "text": "RAP to overcome bad processes\nOne nifty framework for moving from manual processes is RAP or reproducible analytical pipelines. Coined by the UK Government Statistical Service, RAP brings in concepts and practices from data engineering, devops and software carpentry domains to analysts in the public sector.\n\nReproducible Analytical Pipelines (RAPs) are automated statistical and analytical processes. They incorporate elements of software engineering best practice to ensure that the pipelines are reproducible, auditable, efficient, and high quality.\n\nThese practices include:\n\nSubstituting manual steps with code\nUsing modern, open source programming languages\nConverting raw data to statistical output with pipelines / workflows\nUsing version control to keep records of development\nBringing in code review practices\n\nDespite its utility, RAP focuses primarily on converting data from a commonly-managed data store into analytical outputs (reports, tables, models etc). However, in the infrastructure-poor environments of many public sector organisations, data is often inaccessible with no automated process that transforms it from raw data to a form fit for subsequent RAPping. This means the concepts of RAP need to be brought further back into the data analysis process - into the ‘data engineering domain’.\n\n\n\n\n\ngraph LR\nrd[Raw data source] --&gt; lc[Local copy]\nlc --&gt; pr1[Transformation application]\npr1 --&gt; pd[Processed data]\npd --&gt; rep[Reports]\npd --&gt; tab[Tables]\npd --&gt; db[Dashboards]"
  },
  {
    "objectID": "articles/rap-public-sector/index.html#rapping-with-legacy-code",
    "href": "articles/rap-public-sector/index.html#rapping-with-legacy-code",
    "title": "RAPping in the public sector",
    "section": "RAPping with legacy code",
    "text": "RAPping with legacy code\nFor any analyst who has inherited a pre-existent data processing code base replete with manual management, it’s not trivial to rewrite it with RAP principles. Complicated functionality can be difficult to rewrite, and some data extraction steps are challenging. One example of a hard-to-move legacy code base has the following steps:\n\n\n\n\n\nflowchart LR\n    \n    subgraph EXTRACT\n        direction TB\n            oi[Outlook inbox] --&gt; |Download manually|gs[Get and save data]\n            gs --&gt; |Rename file &lt;br&gt; change column types|rd[Raw data store]\n    end\n    \n    subgraph TRANSFORM\n        direction TB\n            e[Raw data store] --&gt;|Manual filename change| sas_read\n            sas_read[Read raw data &lt;br&gt; into SAS format] --&gt; |Manual filename change| sas_trans[Transform data]\n            sas_trans --&gt; data[Processed data]\n    end\n\nfa[feed A] --&gt;|automated &lt;br&gt; delivery| EXTRACT --&gt; TRANSFORM \n\n\n\n\n\n\nIn the following sections, I will briefly sketch one route to RAPping this difficult process. Subsequent posts will cover more nuance in creating local versions of RAP (that include data engineering) and different strategies for making the best use of open source tools and practices. Note, much of the following content can be found in my talk at Statistics New Zealand.\n\nPython as glue\nPython is a modern, multi-paradigm, evolving, open source programming language. It is used widely across many domains - from web development to data science. Due to its breadth of use and popularity, there is an incredible ecosystem of packages. In addition, manual steps like the following can all be done using Python.\n\nChanging file names in scripts to the latest data\nGetting data deliveries from Outlook inboxes\nRunning scripts in order\n\nPython and its rich ecosystem of packages can be used be used as a glue, or interface between different programs. Packages like exhangelib and saspy can connect to APIs and programs like Outlook and SAS respectively. More on how these packages facilitate automation in the following sections.\nAnother aspect of pythonic glue is gluing together a linear pipeline / workflow in a Jupyter notebook. Cells in the notebook can be run in any order manually but using the Run All command sets up a linear execution - cells are run in series giving immediate linear dependency.\n\n\n\n\n\ngraph TD\nf[feed A] --&gt; e[Outlook inbox]\nbeq --&gt; |exchangelib| e\nsc[SAS Code] --&gt; |yaml| pws\n\nsubgraph Jupyter NOTEBOOK\n    dr[Date range] --&gt; beq[[Build email query]]\n    beq --&gt; |exchangelib &lt;br&gt; pandas|gsd[[Get and save data]]\n    gsd --&gt; rd[Raw data store]\n    rd --&gt; |saspy| pws[[Process with SAS]]\n    pws --&gt; pd[Processed data]\nend\n\n\n\n\n\n\nJupyter notebooks also have additional features like:\n\nIncluding documentation alongside code execution - easily updated while pipeline development is still in flux\nIncluding checks (as tables or graphs) as part of the pipeline making the executed notebook a log of the processing run that can be saved for posterity.\nUsing Python’s try execpt can be used to raise errors and stop execution of the pipeline to give the analyst time to correct.\nSince executed outputs are stored in memory so with a sensible structure, it can be quite easy to re-run the notebook from an intermediate point rather than run the entire process again after correcting any issues."
  },
  {
    "objectID": "articles/rap-public-sector/index.html#down-in-the-details",
    "href": "articles/rap-public-sector/index.html#down-in-the-details",
    "title": "RAPping in the public sector",
    "section": "Down in the details",
    "text": "Down in the details\nThe following sections give a little more detail into how exchangelib and saspy help with hard to automate tasks like:\n\nautomatically downloading relevant data sent by email\nautomating SAS code that needs to be run with manual changes (e.g. new file names, data ranges etc.)\n\n\nGetting data with APIs\nUsing email inboxes as a primary data receiver is a common problem since public sector analysts lack the technology infrastructure to transfer data between different organisations. Email ends up being a “solution”. However, using emails for frequent data feeds / frequent processing is not a sustainable process.\nApplication programming interfaces (APIs) allows applications to communicate with each other. The Outlook email program has a rich API behind it called Exchange Web Services (EWS). Applications (like our RAP extract data application) can send EWS queries to push or pull data to Outlook objects like emails, contacts and calendars. The Get and save data functionality can now store the raw dataset as well apply any required transformations - like changing the filenames or data formats.\n\n\n\n\n\ngraph TD\nf[feed A] --&gt; e[Outlook inbox]\nbeq --&gt; |exchangelib| e\nsubgraph EXTRACT    \n    d[Date range] --&gt; beq[[Build email query]]\n    beq --&gt; |exchangelib &lt;br&gt; pandas|gsd[[Get and save data]]\n    gsd --&gt; rd[Raw data]\nend\n\n\n\n\n\n\n\n\nRunning SAS through Python with SASPy\nWhen the SAS codebase is complex, large or both, it’s convenient to instead just make it run without manual changes. Furthermore, this approach allows to incremental refactoring - allowing hard-to-convert code to remain in SAS while moving easier code to Python.\nIt’s worth noting that there are actually two ways of running SAS outside the SAS program: - With a SAS kernel in Jupyter* - Through SASPy\nSASPy is officially supported by SAS, and available as an open source package. The library seems to be well-maintained and well-documented.\n\nAt its core, SASPy is capable of creating a SAS session and sending code to it for execution, as well as returning the output (logs and other output) to the controlling Python script. Yet it is also a powerful generator of SAS code, which means that it offers methods, objects, and syntax for use directly in idiomatic Python that it can then automatically convert to the appropriate SAS language statements for execution. In most cases, SAS procedures or steps are mapped directly to Python methods as a one-to-one equivalent.\n\n\n\nMutating SAS code with Python\nSASPy is able to generate SAS queries from Python commands. However, running existing SAS scripts with part of it needing amendment via Python needs some additional engineering. The easiest solution so far has three main steps:\n\nBreaking up a SAS script into yaml chunks for “immutable” components\n“Mutable” components are created in Python\nThe immutable and mutable are brought together with Python’s f-strings\n\n\n\n\n\n\ngraph TD\nsc[SAS Code] --&gt; |yaml| pws\nsubgraph TRANSFORM\n    rd[Raw data] --&gt; |saspy| pws[[Process with SAS]]\n    pws --&gt; pd[Processed data]\nend"
  },
  {
    "objectID": "articles/maritime-data-enrichment/index.html",
    "href": "articles/maritime-data-enrichment/index.html",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "",
    "text": "The value and challenge of data engineering in the public sector is creating useful real-world analogues as enriched columns or entity tables when commercial data is unavailable. With judicious data engineering, independent data sources can provide myriad perspectives with real-world relevance in analyses and modelling.\nOne of the datasets, I’ve been able to explore is cleaned ship movements (derived from AIS data) through an organisation subscription. The cleaned data provided was split into:\n\nSpatio-temporal point data of the movements - ship tracks\nSpatio-temporal point data of stops - port visits\n\nWhile the bulk of the difficult data cleaning work was already done by the provider, preparing the data for analysis useful for policymakers required additional data wrangling."
  },
  {
    "objectID": "articles/maritime-data-enrichment/index.html#introduction",
    "href": "articles/maritime-data-enrichment/index.html#introduction",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "",
    "text": "The value and challenge of data engineering in the public sector is creating useful real-world analogues as enriched columns or entity tables when commercial data is unavailable. With judicious data engineering, independent data sources can provide myriad perspectives with real-world relevance in analyses and modelling.\nOne of the datasets, I’ve been able to explore is cleaned ship movements (derived from AIS data) through an organisation subscription. The cleaned data provided was split into:\n\nSpatio-temporal point data of the movements - ship tracks\nSpatio-temporal point data of stops - port visits\n\nWhile the bulk of the difficult data cleaning work was already done by the provider, preparing the data for analysis useful for policymakers required additional data wrangling."
  },
  {
    "objectID": "articles/maritime-data-enrichment/index.html#generalising-concepts-from-commerical-container-shipping",
    "href": "articles/maritime-data-enrichment/index.html#generalising-concepts-from-commerical-container-shipping",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Generalising concepts from commerical container shipping",
    "text": "Generalising concepts from commerical container shipping\nShipping lines manage container ships like buses but with greater tactical and strategic agility as they are highly competitive markets. Ships are associated with a vessel fleet that make journeys for a given service to pre-set schedules. For example, a weekly ANL shipping line service (KIX) connecting New Zealand, Australia and Southeastern Asia.\n\n\nService: an ordered set of port visits with an associated fleet that is marketed by a shipping line. In the example above, the KIX service between New Zealand, Australia and Southeastern Asia marketed by the shipping line, ANL.\nJourney: a single rotation done by a ship in the service.\nSchedule: frequency and duration of service.\n\nThe details of commercial offerings from shipping lines cannot be connected with an independent data source like AIS. Instead, we define a series of data enrichments that are conceptually similar and keep a higher level that can generalise across all types of service offerings.\n\nVoyage: an ordered set of international port visits made by a given ship that starts and ends in New Zealand.\nRoute: an ordered set of connected seaboards (sub-regions) based on the ports visited by the ship on a given voyage.\nSchedule: a derived timetable for a ship built from the voyages and associated routes in a given period of time e.g. 2021."
  },
  {
    "objectID": "articles/maritime-data-enrichment/index.html#breaking-journeys---from-visits-to-voyages",
    "href": "articles/maritime-data-enrichment/index.html#breaking-journeys---from-visits-to-voyages",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Breaking journeys - from visits to voyages",
    "text": "Breaking journeys - from visits to voyages\nThe first step in data processing was to puncutate a mass of port visits made by a ship in some unit of time into a voyage. Fortunately, a simple algorithm can separate port visits into voyages for container ships since they run like buses to a fixed schedule.\n\nLike buses, a particular ship can be pulled into another schedule.\nUnlike bus routes, there can be minor changes to the set of visited ports across the year for the same schedule.\n\nThe algorithm identifies a voyage as the sequence of international ports visited between the export and import ports in New Zealand on a single ship journey. Voyages exclude New Zealand ports visited by the ship for cabotage as the policy focus during the COVID-19 pandemic has been on international rather than domestic connectivity.\n\nThe export port is the last New Zealand port before the ship departs for an international port on its outward journey.\nThe import port is the first New Zealand port on the ship’s inward journey.\n\n\nThe algorithm extracts voyages for ships that make at least two separate journeys to New Zealand. A ship that makes a solitary journey to New Zealand from its typical service schedule elsewhere in the world will be not be included. These types of solitary voyages will need additional business logic to isolate the port visits into the ones relevant for the atypical voyage to New Zealand.\nSplitting a contiguous series of port visits into discrete voyages also offers the opportunity of classifying the voyage into a route based on the seaboards of the ports visited in the voyage."
  },
  {
    "objectID": "articles/maritime-data-enrichment/index.html#connecting-seaboards-with-routes",
    "href": "articles/maritime-data-enrichment/index.html#connecting-seaboards-with-routes",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Connecting seaboards with routes",
    "text": "Connecting seaboards with routes\nWe have definted routes as a higher order entity as they can aggregate across voyages in a way that mimics liner shipping services set up by shipping lines but with the added benefit of being insensitive to specific commercial offerings.\nEvery voyage can be classified as belonging to a route based on the (alphabetically ordered) set of unique visited regions. We use the region classification to approximate a seaboard UN geoscheme at the subregion level based on the country of the visited port.\nThe exceptions of this classification are: - dis-aggregating Australia and New Zealand - aggregating Polynesia, Micronesia and Melanesia to Oceania.\nThe ordered set reduces the high variability in the order and number of visited ports in the same region across similar voyages. . For example, a ship that visits Auckland, Melbourne, Brisbane, Shanghai and Busan belongs to the Australia-Eastern Asia-New Zealand route. A ship that visits Tauranga, Sydney, Hong Kong, Ningbo and Tokyo will also be part of the same route Australia-Eastern Asia-New Zealand."
  },
  {
    "objectID": "articles/maritime-data-enrichment/index.html#adding-enrichment-to-ship-tracks",
    "href": "articles/maritime-data-enrichment/index.html#adding-enrichment-to-ship-tracks",
    "title": "Enriching independent data with industry-relevant concepts",
    "section": "Adding enrichment to ship tracks",
    "text": "Adding enrichment to ship tracks\nSince the port visits are a reduced version of the ship tracks focused on stop points, they are the first step for data enrichment. However, both data sets are complementary and provide different perspectives of ship movement.\n\nThe enrichment of voyages and routes in port visits can be joined to the spatio-temporal ship tracks data on ship name. Fanout is removed with a time filter - only tracks within the time span of a given voyage are kept in the data."
  },
  {
    "objectID": "articles/practical-docs/index.html",
    "href": "articles/practical-docs/index.html",
    "title": "Writing practical documentation",
    "section": "",
    "text": "“Documentation is communication”. And communication is all about matching the relevant content to the audience. Jean-Luc Doumont, in his opus Trees, maps and theorems, advises writers to understand the audience based on the need for content vs. context.\nPotential audiences fall along some spectrum between specialises and generalists. For technical documentation however, the split is reasonably clear. Specialists want more specificity and jargon for precise understanding while non-specialists more contextual information and non-technical terminology.\n\nspecialists - need detail, technical terms.\nnon-specialists - need background, interpretation, non-technical terms.\n\nDoumont’s insight is an often-ignored audience need: readers can be close or far (in space or time) from the work. The current technical analyst will be a specialist, primary reader while one who will be onboarded to the project in the future will be specialist, secondary reader. A current manager, on the hand, will be a non-specialist, primary reader.\n\nprimary readers - close to the situation (here and now) and require less context.\nsecondary readers - reading far from the situation and require more context.\n\nTechnical documentation should cater to all four groups split along the axis of context and content. Once again, Doumont’s advice is sharp: cater to all audience types with fractal documents. A fractal document follows the same pattern at every scale (document, chapter, section) of a global component followed by details.\nGlobal components contextualise, summarise and interpret the main point(s) for the general readers without losing the specialised readers while details anticipate questions from specialised readers. With this clarity in structure readers can directly access the information they need without wading through irrelevant cruft.\n\nDoumont’s advice concerns the “who” and the “how” while the GUT of documentation provides a useful high level framework for the “what”. This framework classifies different types of documentation based on use. For projects which build analyses as reproducible analytical pipelines, understanding-oriented documentation is the first cab off the ranks as it provides value to widest audience.\n\nUnderstanding-oriented documentation starts with contextual information e.g. why the process exists, how it is useful and valuable to customers, common issues and key stakeholders. This introduction should be understandable by a manager helping them understand the value of the project with details for prioritising and negotiating resource. It’s also an easy read for the manager when their clout is needed for dealing with any inter-personal conflict about the work.\nAll subsequent chapters are intended for a technical audience, starting with a technical overview. The overview is suitable for anyone in the technical team (specialised secondary readers) interested in learning more about the project. This view can contain details about the core algorithm (e.g. a machine learning technique), software design pattern used in the codebase etc. The information is relevant to the project but also general enough to be interesting to other technical analysts who won’t directly use the code. Sparing details about running a reporting process can be included here though it’s better suited to the README as an easier access point for quick updates.\nFor complex projects, additional chapters after the technical overview can describe specific technical details or administrative information (e.g. procurement, raising issues etc). This view is meant for the specialised primary reader and can be written as an information-oriented reference e.g. describing the inputs, outputs or any caveats in interpretation or use.\n\nWriting good documentation is a skill most of us neglect. The rewards seem lower than the effort especially when there isn’t a clear pattern to follow. The format of a fractal document with clear understanding of readers not only makes writing standardised documentation easier but also technical writing in general. When approaching analyses with a data product perspective, easy to read reports are a cornerstone data prouduct for business stakeholders."
  }
]